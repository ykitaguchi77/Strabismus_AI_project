{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Strabismus_AI_project/blob/main/Strabismus_3to6_class_RepVGG_local_1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU"
      },
      "source": [
        "#**Strabismus: RepVGG_rangerAdabrief**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3-class to 6-class**"
      ],
      "metadata": {
        "id": "UXJqctl5YZCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#課題\n",
        "#triplet lossの導入？\n",
        "#focal lossの導入\n",
        "#Boundaryの取り扱いをどうするか？？→50％斜視、50％正常とする？？そして判定結果のパーセンテージでクラス分けする？？"
      ],
      "metadata": {
        "id": "wiUIcBkYbJQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3-class\n",
        "- cont\n",
        "- exo\n",
        "- eso\n",
        "\n",
        "6-class\n",
        "- exo → exo 1.0\n",
        "- exo-boundary: 1mm前後の外斜視 →　exo 0.5,cont 0.5\n",
        "- cont → cont 1.0\n",
        "- eso-boundary: 1mm前後の内斜視 → eso 0.5, cont 0.5\n",
        "- eso → eso 1.0\n",
        "- inadequate: 周辺視、片眼のみの写真、片眼あるいは両眼の瞳孔が出ていない、眼位写真以外のもの → inadequate 1.0\n",
        "\n",
        "3-classのモデルから転移学習を行う。\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NWDJ_PhsYfqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989176f6-49fc-4d9e-e6bd-366e05ce3ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n3-class\\n- cont\\n- exo\\n- eso\\n\\n6-class\\n- exo → exo 1.0\\n- exo-boundary: 1mm前後の外斜視 →\\u3000exo 0.5,cont 0.5\\n- cont → cont 1.0\\n- eso-boundary: 1mm前後の内斜視 → eso 0.5, cont 0.5\\n- eso → eso 1.0\\n- inadequate: 周辺視、片眼のみの写真、片眼あるいは両眼の瞳孔が出ていない、眼位写真以外のもの → inadequate 1.0\\n\\n3-classのモデルから転移学習を行う。\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "29bff432-26e8-44dd-ee65-45ff7904fb1d"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "import csv\n",
        "import cv2\n",
        "import random\n",
        "import sys\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\"\"\"                                \n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: You are using pip version 22.0.2; however, version 22.0.3 is available.\n",
            "You should consider upgrading via the 'c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_optimizer in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch_optimizer) (1.10.1+cu102)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=1.5.0->torch_optimizer) (4.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\ykita\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     31\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mion()   \u001b[38;5;66;03m# interactive mode\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**空のデータセットフォルダを作成**"
      ],
      "metadata": {
        "id": "e-XfwYta_lAD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg3cJNph6yia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab44086c-8aee-42b9-ef7e-f82222a0b160"
      },
      "source": [
        "'''\n",
        "データの構造\n",
        "---Dataset_1to100_eval------ cont\n",
        "                        |--- exo\n",
        "                        |--- eso\n",
        "                        |--- eso-to --- eso-exo\n",
        "                        |            |- eso-cont\n",
        "                        |            |- eso-exo-boundary\n",
        "                        |            |- eso-eso-boundary\n",
        "                        |            |- eso-inadequate\n",
        "                        |--- exo-to --- exo-eso\n",
        "                        |            |- exo-cont\n",
        "                        |            |- exo-exo-boundary\n",
        "                        |            |- exo-eso-boundary\n",
        "                        |            |- exo-inadequate\n",
        "                        |--- cont-to --- cont-eso\n",
        "                        |            |- cont-exo\n",
        "                        |            |- cont-exo-boundary\n",
        "                        |            |- cont-eso-boundary\n",
        "                        |            |- cont-inadequate                         \n",
        "                        |--- corrected --- cont\n",
        "                                        |- exo\n",
        "                                        |- exo-boundary\n",
        "                                        |- eso\n",
        "                                        |- eso-boundary\n",
        "                                        |- inadequate\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nデータの構造\\n---Dataset_1to100_eval------ cont\\n                        |--- exo\\n                        |--- eso\\n                        |--- eso-to --- eso-exo\\n                        |            |- eso-cont\\n                        |            |- eso-exo-boundary\\n                        |            |- eso-eso-boundary\\n                        |            |- eso-inadequate\\n                        |--- exo-to --- exo-eso\\n                        |            |- exo-cont\\n                        |            |- exo-exo-boundary\\n                        |            |- exo-eso-boundary\\n                        |            |- exo-inadequate\\n                        |--- cont-to --- cont-eso\\n                        |            |- cont-exo\\n                        |            |- cont-exo-boundary\\n                        |            |- cont-eso-boundary\\n                        |            |- cont-inadequate                         \\n                        |--- corrected --- cont\\n                                        |- exo\\n                                        |- exo-boundary\\n                                        |- eso\\n                                        |- eso-boundary\\n                                        |- inadequate\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#eval_folder_tree_txt\n",
        "\"\"\"\n",
        "cont\n",
        "exo\n",
        "eso\n",
        "eso_boundary\n",
        "exo_boundary\n",
        "inadequate\n",
        "eso_to\n",
        "\teso_exo\n",
        "\teso_cont\n",
        "\teso_exo_boundary\n",
        "\teso_eso_boundary\n",
        "\teso_inadequate\n",
        "exo_to\n",
        "\texo_eso\n",
        "\texo_cont\n",
        "\texo_exo_boundary\n",
        "\texo_eso_boundary\n",
        "\texo_inadequate\n",
        "cont_to\n",
        "\tcont_exo\n",
        "\tcont_exo_boundary\n",
        "\tcont_eso\n",
        "\tcont_eso_boundary\n",
        "\tcont_inadequate\n",
        "exo_boundary_to\n",
        "\texo_boundary_cont\n",
        "\texo_boundary_exo\n",
        "\texo_boundary_eso\n",
        "\texo_boundary_eso_boundary\n",
        "\texo_boundary_inadequate\n",
        "eso_boundary_to\n",
        "\teso_boundary_cont\n",
        "\teso_boundary_exo\n",
        "\teso_boundary_eso\n",
        "\teso_boundary_exo_boundary\n",
        "\teso_boundary_inadequate\n",
        "inadequate_to\n",
        "\tinadequate_cont\n",
        "\tinadequate_exo\n",
        "\tinadequate_eso\n",
        "\tinadequate_exo_boundary\n",
        "\tinadequate_eso_boundary\n",
        "corrected\n",
        "\tcorrected_exo\n",
        "\tcorrected_exo_boundary\n",
        "\tcorrected_eso\n",
        "\tcorrected_eso_boundary\n",
        "\tcorrected_inadequate\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh02-bfi10CD",
        "outputId": "03656d36-02ab-4565-bd52-6abcaac411f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncont\\nexo\\neso\\neso_boundary\\nexo_boundary\\ninadequate\\neso_to\\n\\teso_exo\\n\\teso_cont\\n\\teso_exo_boundary\\n\\teso_eso_boundary\\n\\teso_inadequate\\nexo_to\\n\\texo_eso\\n\\texo_cont\\n\\texo_exo_boundary\\n\\texo_eso_boundary\\n\\texo_inadequate\\ncont_to\\n\\tcont_exo\\n\\tcont_exo_boundary\\n\\tcont_eso\\n\\tcont_eso_boundary\\n\\tcont_inadequate\\nexo_boundary_to\\n\\texo_boundary_cont\\n\\texo_boundary_exo\\n\\texo_boundary_eso\\n\\texo_boundary_eso_boundary\\n\\texo_boundary_inadequate\\neso_boundary_to\\n\\teso_boundary_cont\\n\\teso_boundary_exo\\n\\teso_boundary_eso\\n\\teso_boundary_exo_boundary\\n\\teso_boundary_inadequate\\ninadequate_to\\n\\tinadequate_cont\\n\\tinadequate_exo\\n\\tinadequate_eso\\n\\tinadequate_exo_boundary\\n\\tinadequate_eso_boundary\\ncorrected\\n\\tcorrected_exo\\n\\tcorrected_exo_boundary\\n\\tcorrected_eso\\n\\tcorrected_eso_boundary\\n\\tcorrected_inadequate\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Textファイルの記載に沿ってフォルダを作成**"
      ],
      "metadata": {
        "id": "PVsFaFpy32l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dst_dir = r\"F:\\Strabismus\\Dataset_101to200_eval\"\n",
        "\n",
        "def confirm():\n",
        "    dic={'y':True,'yes':True,'n':False,'no':False}\n",
        "    while True:\n",
        "        try:\n",
        "            return dic[input('すでにフォルダがあります。上書きしますか? [Y]es/[N]o >> ').lower()]\n",
        "        except:\n",
        "            pass\n",
        "        print('Error! Input again.')\n",
        "\n",
        "#保存先フォルダを作成\n",
        "if os.path.exists(dst_dir):\n",
        "    if confirm():\n",
        "        shutil.rmtree(dst_dir)\n",
        "    else:\n",
        "        sys.exit()\n",
        "\n",
        "os.makedirs(dst_dir)\n",
        "\n",
        "#作業フォルダの設定\n",
        "os.chdir(dst_dir)\n",
        "\n",
        "#テキストファイルの構造通りにフォルダを作成する\n",
        "#参考サイト：https://fastclassinfo.com/entry/python_makefolders/\n",
        "#プログラム1｜ライブラリの設定\n",
        "#import os\n",
        " \n",
        "#プログラム2｜フォルダパスの取得\n",
        "path = r\"F:\\Strabismus\\eval_folder_tree.txt\"\n",
        "fullpath = os.getcwd()\n",
        " \n",
        "#プログラム3｜ファイル格納リスト\n",
        "folderlist = []\n",
        " \n",
        "#プログラム4｜テキストファイル内のデータをリストへ格納\n",
        "with open(path, encoding='shift-jis') as lines:\n",
        "    for line in lines:\n",
        "        line = line.replace('\\n','')\n",
        "        folderlist.append(line.split('\\t'))\n",
        " \n",
        "#プログラム5｜作成したいフォルダを1つずつ編集\n",
        "for i, folders in enumerate(folderlist):\n",
        "    list1 = []\n",
        " \n",
        "    #プログラム6｜データがなければ、一つ前のパスを入れる\n",
        "    for j, folder in enumerate(folders):\n",
        "        if folder != '':\n",
        "            list1.append(folderlist[i][j])\n",
        "        else:\n",
        "            list1.append(folderlist[i-1][j])\n",
        "    folderlist[i] = list1\n",
        " \n",
        "    #プログラム7｜folderpathにフルパスを作る\n",
        "    folderpath = fullpath +'\\\\' + '\\\\'.join(folderlist[i])\n",
        "    \n",
        "    # プログラム8｜同じ名前のフォルダが存在しなければ、フォルダを作成\n",
        "    if os.path.exists(folderpath) == False:\n",
        "        os.makedirs(folderpath)\n",
        "\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnIvaUAI_44K",
        "outputId": "6ad6d943-4cee-4c8c-c09d-e8d9c5c89f87"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cont',\n",
              " 'exo',\n",
              " 'eso',\n",
              " 'eso_boundary',\n",
              " 'exo_boundary',\n",
              " 'inadequate',\n",
              " 'eso_to',\n",
              " 'exo_to',\n",
              " 'cont_to',\n",
              " 'exo_boundary_to',\n",
              " 'eso_boundary_to',\n",
              " 'inadequate_to',\n",
              " 'corrected']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ファイル数をカウント"
      ],
      "metadata": {
        "id": "T7jkALUAWMtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#それぞれの症例数をカウント\n",
        "def count(arg):\n",
        "    #print(arg)\n",
        "    dir = os.getcwd()\n",
        "    #print(glob.glob(dir+\"/**/\"+arg,recursive=True))\n",
        "    files_dir = glob.glob(dir+\"/**/\"+arg+\"/*\",recursive=True)\n",
        "    count = (len([name for name in files_dir if os.path.isfile(name)]))\n",
        "    #print(count)\n",
        "    #print(\"\")\n",
        "    return count\n",
        "\n",
        "dst_dir = dst_dir #dst_dirは前のカラムで設定済\n",
        "os.chdir(dst_dir)\n",
        "\n",
        "cont_num = count(\"cont\") + count(\"exo_cont\") + count(\"eso_cont\") + count(\"exo_boundary_cont\")+ count(\"eso_boundary_cont\") + count(\"inadequate_cont\")\n",
        "exo_num = count(\"exo\") + count(\"cont_exo\") + count(\"eso_exo\") + count(\"exo_boundary_exo\") + count(\"eso_boundary_exo\")+ count(\"inadequate_exo\")\n",
        "eso_num = count(\"eso\") + count(\"cont_eso\") + count(\"exo_eso\") + count(\"exo_boundary_eso\") + count(\"eso_boundary_eso\") +count(\"inadequate_eso\")\n",
        "eso_boundary_num = count(\"eso_boundary\") + count(\"cont_eso_boundary\") + count(\"exo_eso_boundary\")+ count(\"eso_eso_boundary\") + count(\"exo_boundary_eso_boundary\") + count(\"inadequate_eso_boundary\")\n",
        "exo_boundary_num = count(\"exo_boundary\") + count(\"cont_exo_boundary\") + count(\"exo_exo_boundary\")+ count(\"eso_exo_boundary\") + count(\"eso_boundary_exo_boundary\") + count(\"inadequate_exo_boundary\")\n",
        "inadequate_num = count(\"inadequate\") + count(\"exo_inadequate\") + count(\"eso_inadequate\") + count(\"cont_inadequate\")+ count(\"exo_boundary_inadequate\") + count(\"eso_boundary_inadequate\")\n",
        "\n",
        "print(\"cont: \"+str(cont_num))\n",
        "print(\"exo: \"+str(exo_num))\n",
        "print(\"eso: \"+str(eso_num))\n",
        "print(\"exo_boundary: \"+str(exo_boundary_num))\n",
        "print(\"eso_boundary: \"+str(eso_boundary_num))\n",
        "print(\"inadequate: \"+str(inadequate_num))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgKpDDI9mKs7",
        "outputId": "2a0157f7-be8e-42ab-d593-741b2340b1be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cont: 0\n",
            "exo: 0\n",
            "eso: 0\n",
            "exo_boundary: 0\n",
            "eso_boundary: 0\n",
            "inadequate: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#それぞれからcorrectフォルダに移すスクリプトが必要（後で書く）"
      ],
      "metadata": {
        "id": "I0En7ysfVkjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "outputId": "230c6814-f52b-4ba9-e45e-f1fb2a929aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "data_dir = r\"F:\\Strabismus\\Dataset_1to100_eval\\corrected\"\n",
        "\n",
        "dataset = datasets.ImageFolder(data_dir)\n",
        "print(dataset.classes)\n",
        "print(dataset.class_to_idx)\n",
        "\n",
        "\n",
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        Expand2square((0,0,0)),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.85,1.0)), \n",
        "        transforms.RandomHorizontalFlip(),     \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "val_transform = transforms.Compose([\n",
        "        Expand2square((0,0,0)),\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "test_transform = transforms.Compose([\n",
        "        Expand2square((0,0,0)),\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#データセットをシャッフルして分割\n",
        "#https://qiita.com/sin9270/items/c6023453e00bfead9e9f\n",
        "#https://pystyle.info/pytorch-split-dataset/\n",
        "\n",
        "\n",
        "class MySubset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, indices, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[self.indices[idx]]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "\n",
        "\n",
        "# Train:val:testに 7:2:1 の割合で分割する(ランダムに分割）。\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "train_val_size = int(0.9 * len(dataset))\n",
        "indices = np.arange(len(dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_dataset = MySubset(dataset, indices[:train_size], train_transform)\n",
        "val_dataset = MySubset(dataset, indices[train_size:train_val_size], val_transform)\n",
        "test_dataset = MySubset(dataset, indices[train_val_size:], test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 4, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False)\n",
        "\n",
        "#class_names = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "class_names = dataset.classes\n",
        "\n",
        "print(f\"full: {len(dataset)} -> train: {len(train_dataset)}, val: {len(val_dataset)},test: {len(test_dataset)}\")\n",
        "print(\"class_names -> \"+str(dataset.classes))\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(train_loader))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "        \n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            \"\"\"\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"outputs: \"+str(outputs))\n",
        "            print(\"preds: \"+str(preds))\n",
        "            print(\"loss: \"+str(loss))\n",
        "            \"\"\"\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "            \n",
        "        print()   \n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "           \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        #####################\n",
        "        # test the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, test_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "           \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        test_acc = running_corrects.item()/len(test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(num_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}' +'\\n'\n",
        "                     f'test_acc: {test_acc:.5f}') \n",
        "\n",
        "        \n",
        "        print(print_msg)\n",
        "        \n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['corrected_cont', 'corrected_eso', 'corrected_eso-boundary', 'corrected_exo', 'corrected_exo-boundary', 'corrected_inadequate']\n",
            "{'corrected_cont': 0, 'corrected_eso': 1, 'corrected_eso-boundary': 2, 'corrected_exo': 3, 'corrected_exo-boundary': 4, 'corrected_inadequate': 5}\n",
            "full: 844 -> train: 590, val: 169,test: 85\n",
            "class_names -> ['corrected_cont', 'corrected_eso', 'corrected_eso-boundary', 'corrected_exo', 'corrected_exo-boundary', 'corrected_inadequate']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaWr9bLXpAz"
      },
      "source": [
        "#**Define RepVGG**\n",
        "https://github.com/DingXiaoH/RepVGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW9Kd5meE3JK"
      },
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226jazomBPzw"
      },
      "source": [
        "#RepVGG-A2のpretrained modelをダウンロード(1回ダウンロードすればあとは省略可)\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "file_id = '1PvtYTOX4gd-1VHX8LoT7s6KIyfTKOf8G'\n",
        "destination = r\"F:\\Strabismus/RepVGG-A2.pth\"\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_APCDCmTE3PZ",
        "outputId": "8059d7ec-8d8a-46c4-d4c1-eeaace790d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "source": [
        "#deploy RepVGG-A2\n",
        "\"\"\"\n",
        "train_model = create_RepVGG_A2(deploy=False)\n",
        "train_model.load_state_dict(torch.load('/content/drive/MyDrive/Deep_learning/RepVGG-A2-train.pth'))   \n",
        "model_ft = repvgg_model_convert(train_model, create_RepVGG_A2, save_path='/content/drive/MyDrive/Deep_learning/repvgg-A2-deploy.pth')\n",
        "\"\"\"\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "\n",
        "#use pretrained model\n",
        "\n",
        "PATH = r\"F:\\Strabismus/RepVGG-A2.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#model_ft.load_state_dict(torch.load(r\"F:\\Strabismus/RepVGG-A2.pth\"))   \n",
        "#model_ft.load_state_dict(torch.load('/content/RepVGG-A2.pth'))   \n",
        "num_ftrs = model_ft.linear.in_features\n",
        "model_ft.linear = nn.Linear(num_ftrs, 6)\n",
        "\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "\n",
        "!pip install ranger-adabelief==0.1.0\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#use pretrained model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mStrabismus/RepVGG-A2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m model_ft\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#model_ft.load_state_dict(torch.load(r\"F:\\Strabismus/RepVGG-A2.pth\"))   \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#model_ft.load_state_dict(torch.load('/content/RepVGG-A2.pth'))   \u001b[39;00m\n\u001b[0;32m     16\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m model_ft\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39min_features\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\serialization.py:608\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\serialization.py:777\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 777\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '<'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeHuwTdcJcDj"
      },
      "source": [
        "\"#モデルのサマリー（省略可）\n",
        "from torchsummary import summary\n",
        "model_ft.to(device)\n",
        "summary(model_ft, (3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBXuY1JlOY"
      },
      "source": [
        "#モデルの表示（省略可）\n",
        "print(model_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvx3RgvOPNv0"
      },
      "source": [
        "#**Convnetの調整**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1du9txkSPN20"
      },
      "source": [
        "#**訓練と評価**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UpcgsmxPN6l",
        "outputId": "fe09fe2e-c338-4eb7-e4ed-618ad6fb851f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=30, num_epochs=150)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "----------\n",
            "\n",
            "Epoch: [  0/150] \n",
            "train_loss: 1.13521 train_acc: 0.60508\n",
            "valid_loss: 0.90446 valid_acc: 0.71598\n",
            "test_acc: 0.65882\n",
            "Validation loss decreased (inf --> 0.904461).  Saving model ...\n",
            "\n",
            "Epoch 2/150\n",
            "----------\n",
            "\n",
            "Epoch: [  1/150] \n",
            "train_loss: 0.97626 train_acc: 0.68644\n",
            "valid_loss: 0.80953 valid_acc: 0.71598\n",
            "test_acc: 0.68235\n",
            "Validation loss decreased (0.904461 --> 0.809526).  Saving model ...\n",
            "\n",
            "Epoch 3/150\n",
            "----------\n",
            "\n",
            "Epoch: [  2/150] \n",
            "train_loss: 0.81161 train_acc: 0.72712\n",
            "valid_loss: 0.79026 valid_acc: 0.76331\n",
            "test_acc: 0.71765\n",
            "Validation loss decreased (0.809526 --> 0.790256).  Saving model ...\n",
            "\n",
            "Epoch 4/150\n",
            "----------\n",
            "\n",
            "Epoch: [  3/150] \n",
            "train_loss: 0.77134 train_acc: 0.74237\n",
            "valid_loss: 0.98738 valid_acc: 0.63314\n",
            "test_acc: 0.58824\n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "Epoch 5/150\n",
            "----------\n",
            "\n",
            "Epoch: [  4/150] \n",
            "train_loss: 0.78558 train_acc: 0.74068\n",
            "valid_loss: 0.88148 valid_acc: 0.71006\n",
            "test_acc: 0.67059\n",
            "EarlyStopping counter: 2 out of 30\n",
            "\n",
            "Epoch 6/150\n",
            "----------\n",
            "\n",
            "Epoch: [  5/150] \n",
            "train_loss: 0.70173 train_acc: 0.76949\n",
            "valid_loss: 0.73056 valid_acc: 0.76331\n",
            "test_acc: 0.68235\n",
            "Validation loss decreased (0.790256 --> 0.730563).  Saving model ...\n",
            "\n",
            "Epoch 7/150\n",
            "----------\n",
            "\n",
            "Epoch: [  6/150] \n",
            "train_loss: 0.65044 train_acc: 0.77458\n",
            "valid_loss: 0.83629 valid_acc: 0.74556\n",
            "test_acc: 0.68235\n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "Epoch 8/150\n",
            "----------\n",
            "\n",
            "Epoch: [  7/150] \n",
            "train_loss: 0.58934 train_acc: 0.79661\n",
            "valid_loss: 0.77681 valid_acc: 0.72189\n",
            "test_acc: 0.71765\n",
            "EarlyStopping counter: 2 out of 30\n",
            "\n",
            "Epoch 9/150\n",
            "----------\n",
            "\n",
            "Epoch: [  8/150] \n",
            "train_loss: 0.62485 train_acc: 0.77966\n",
            "valid_loss: 0.68223 valid_acc: 0.82249\n",
            "test_acc: 0.74118\n",
            "Validation loss decreased (0.730563 --> 0.682233).  Saving model ...\n",
            "\n",
            "Epoch 10/150\n",
            "----------\n",
            "\n",
            "Epoch: [  9/150] \n",
            "train_loss: 0.50864 train_acc: 0.82203\n",
            "valid_loss: 1.07345 valid_acc: 0.67456\n",
            "test_acc: 0.61176\n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "Epoch 11/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 10/150] \n",
            "train_loss: 0.49243 train_acc: 0.82203\n",
            "valid_loss: 1.05858 valid_acc: 0.73964\n",
            "test_acc: 0.65882\n",
            "EarlyStopping counter: 2 out of 30\n",
            "\n",
            "Epoch 12/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 11/150] \n",
            "train_loss: 0.47321 train_acc: 0.82712\n",
            "valid_loss: 0.91810 valid_acc: 0.69822\n",
            "test_acc: 0.69412\n",
            "EarlyStopping counter: 3 out of 30\n",
            "\n",
            "Epoch 13/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 12/150] \n",
            "train_loss: 0.62156 train_acc: 0.79831\n",
            "valid_loss: 1.39508 valid_acc: 0.63905\n",
            "test_acc: 0.61176\n",
            "EarlyStopping counter: 4 out of 30\n",
            "\n",
            "Epoch 14/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 13/150] \n",
            "train_loss: 0.42500 train_acc: 0.85085\n",
            "valid_loss: 1.54262 valid_acc: 0.65680\n",
            "test_acc: 0.61176\n",
            "EarlyStopping counter: 5 out of 30\n",
            "\n",
            "Epoch 15/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 14/150] \n",
            "train_loss: 0.49094 train_acc: 0.83898\n",
            "valid_loss: 0.89669 valid_acc: 0.73964\n",
            "test_acc: 0.72941\n",
            "EarlyStopping counter: 6 out of 30\n",
            "\n",
            "Epoch 16/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 15/150] \n",
            "train_loss: 0.43608 train_acc: 0.85593\n",
            "valid_loss: 1.03613 valid_acc: 0.74556\n",
            "test_acc: 0.71765\n",
            "EarlyStopping counter: 7 out of 30\n",
            "\n",
            "Epoch 17/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 16/150] \n",
            "train_loss: 0.42619 train_acc: 0.85085\n",
            "valid_loss: 0.85137 valid_acc: 0.78107\n",
            "test_acc: 0.72941\n",
            "EarlyStopping counter: 8 out of 30\n",
            "\n",
            "Epoch 18/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 17/150] \n",
            "train_loss: 0.38240 train_acc: 0.87458\n",
            "valid_loss: 1.14867 valid_acc: 0.69822\n",
            "test_acc: 0.62353\n",
            "EarlyStopping counter: 9 out of 30\n",
            "\n",
            "Epoch 19/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 18/150] \n",
            "train_loss: 0.31367 train_acc: 0.89153\n",
            "valid_loss: 0.97610 valid_acc: 0.72189\n",
            "test_acc: 0.60000\n",
            "EarlyStopping counter: 10 out of 30\n",
            "\n",
            "Epoch 20/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 19/150] \n",
            "train_loss: 0.34816 train_acc: 0.87458\n",
            "valid_loss: 1.09538 valid_acc: 0.69822\n",
            "test_acc: 0.69412\n",
            "EarlyStopping counter: 11 out of 30\n",
            "\n",
            "Epoch 21/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 20/150] \n",
            "train_loss: 0.38805 train_acc: 0.87966\n",
            "valid_loss: 1.05636 valid_acc: 0.73964\n",
            "test_acc: 0.71765\n",
            "EarlyStopping counter: 12 out of 30\n",
            "\n",
            "Epoch 22/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 21/150] \n",
            "train_loss: 0.27708 train_acc: 0.90678\n",
            "valid_loss: 0.89497 valid_acc: 0.79290\n",
            "test_acc: 0.78824\n",
            "EarlyStopping counter: 13 out of 30\n",
            "\n",
            "Epoch 23/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 22/150] \n",
            "train_loss: 0.40485 train_acc: 0.87797\n",
            "valid_loss: 1.24635 valid_acc: 0.80473\n",
            "test_acc: 0.80000\n",
            "EarlyStopping counter: 14 out of 30\n",
            "\n",
            "Epoch 24/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 23/150] \n",
            "train_loss: 0.23455 train_acc: 0.91525\n",
            "valid_loss: 0.87752 valid_acc: 0.76331\n",
            "test_acc: 0.75294\n",
            "EarlyStopping counter: 15 out of 30\n",
            "\n",
            "Epoch 25/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 24/150] \n",
            "train_loss: 0.33982 train_acc: 0.87458\n",
            "valid_loss: 0.98508 valid_acc: 0.72781\n",
            "test_acc: 0.67059\n",
            "EarlyStopping counter: 16 out of 30\n",
            "\n",
            "Epoch 26/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 25/150] \n",
            "train_loss: 0.32600 train_acc: 0.88305\n",
            "valid_loss: 0.92349 valid_acc: 0.76923\n",
            "test_acc: 0.74118\n",
            "EarlyStopping counter: 17 out of 30\n",
            "\n",
            "Epoch 27/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 26/150] \n",
            "train_loss: 0.23416 train_acc: 0.92203\n",
            "valid_loss: 0.88600 valid_acc: 0.74556\n",
            "test_acc: 0.80000\n",
            "EarlyStopping counter: 18 out of 30\n",
            "\n",
            "Epoch 28/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 27/150] \n",
            "train_loss: 0.25556 train_acc: 0.92373\n",
            "valid_loss: 3.31956 valid_acc: 0.71006\n",
            "test_acc: 0.74118\n",
            "EarlyStopping counter: 19 out of 30\n",
            "\n",
            "Epoch 29/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 28/150] \n",
            "train_loss: 0.24071 train_acc: 0.92373\n",
            "valid_loss: 1.39902 valid_acc: 0.77515\n",
            "test_acc: 0.80000\n",
            "EarlyStopping counter: 20 out of 30\n",
            "\n",
            "Epoch 30/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 29/150] \n",
            "train_loss: 0.27454 train_acc: 0.91864\n",
            "valid_loss: 0.93384 valid_acc: 0.78107\n",
            "test_acc: 0.81176\n",
            "EarlyStopping counter: 21 out of 30\n",
            "\n",
            "Epoch 31/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 30/150] \n",
            "train_loss: 0.20925 train_acc: 0.92712\n",
            "valid_loss: 14.79079 valid_acc: 0.77515\n",
            "test_acc: 0.72941\n",
            "EarlyStopping counter: 22 out of 30\n",
            "\n",
            "Epoch 32/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 31/150] \n",
            "train_loss: 0.31519 train_acc: 0.90339\n",
            "valid_loss: 1.22662 valid_acc: 0.76923\n",
            "test_acc: 0.74118\n",
            "EarlyStopping counter: 23 out of 30\n",
            "\n",
            "Epoch 33/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 32/150] \n",
            "train_loss: 0.24258 train_acc: 0.92712\n",
            "valid_loss: 8.28140 valid_acc: 0.73373\n",
            "test_acc: 0.75294\n",
            "EarlyStopping counter: 24 out of 30\n",
            "\n",
            "Epoch 34/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 33/150] \n",
            "train_loss: 0.22966 train_acc: 0.92712\n",
            "valid_loss: 18.22708 valid_acc: 0.73964\n",
            "test_acc: 0.74118\n",
            "EarlyStopping counter: 25 out of 30\n",
            "\n",
            "Epoch 35/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 34/150] \n",
            "train_loss: 0.16648 train_acc: 0.94915\n",
            "valid_loss: 12.40163 valid_acc: 0.76331\n",
            "test_acc: 0.69412\n",
            "EarlyStopping counter: 26 out of 30\n",
            "\n",
            "Epoch 36/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 35/150] \n",
            "train_loss: 0.14857 train_acc: 0.94915\n",
            "valid_loss: 0.96964 valid_acc: 0.78698\n",
            "test_acc: 0.75294\n",
            "EarlyStopping counter: 27 out of 30\n",
            "\n",
            "Epoch 37/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 36/150] \n",
            "train_loss: 0.18778 train_acc: 0.94746\n",
            "valid_loss: 1.22961 valid_acc: 0.76923\n",
            "test_acc: 0.70588\n",
            "EarlyStopping counter: 28 out of 30\n",
            "\n",
            "Epoch 38/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 37/150] \n",
            "train_loss: 0.24518 train_acc: 0.91186\n",
            "valid_loss: 1.46333 valid_acc: 0.74556\n",
            "test_acc: 0.70588\n",
            "EarlyStopping counter: 29 out of 30\n",
            "\n",
            "Epoch 39/150\n",
            "----------\n",
            "\n",
            "Epoch: [ 38/150] \n",
            "train_loss: 0.17163 train_acc: 0.95254\n",
            "valid_loss: 18.17594 valid_acc: 0.76923\n",
            "test_acc: 0.71765\n",
            "EarlyStopping counter: 30 out of 30\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6D7vVaHPN98"
      },
      "source": [
        "#**Calculate Accuracy**\n",
        "・True positive (TN)<br>\n",
        "・False positive (FP)<br>\n",
        "・True negative (TN)<br>\n",
        "・False negative (FN)<br>\n",
        "\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)<br>\n",
        "Precision = TP/(FP + TP) ※positive predictive value<br>\n",
        "Recall = TP/(TP + FN)　※sensitivity<br>\n",
        "Specificity = TN/(FP + TN)<br>\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Fzf34FPOGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "58393c17-9c27-45ad-e3a6-15aa63e18a5a"
      },
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 1.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-b7dfd1faf622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# visualize the loss as the network trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1P_oH6gQQG9"
      },
      "source": [
        "visualize_model(model_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4xXKF2POJ8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SShmQdShtkMR"
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = r\"F:\\Strabismus\\Dataset_1to100_eval/base_model.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BVnnfPrTUmla"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWpxojQitkOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f294afc-1446-4f56-dfb3-cddfafcba9e8"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "num_ftrs = model_ft.linear.in_features\n",
        "model_ft.linear = nn.Linear(num_ftrs, 6)\n",
        "\n",
        "PATH = r\"F:\\Strabismus\\Dataset_1to100_eval/base_model.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_ft)"
      ],
      "metadata": {
        "id": "BBYIqYyGBPd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XQuhBl0joHM"
      },
      "source": [
        "#**Evaluation using confusion matrix, and draw ROC curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "opNUHRr2jido",
        "outputId": "a2346e13-6a61-416e-ea78-3b932eaf14ac"
      },
      "source": [
        "import statistics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def make_cm(matrix, columns):\n",
        "    # matrix numpy配列\n",
        "\n",
        "    # columns 項目名リスト\n",
        "    n = len(columns)\n",
        "\n",
        "    # '正解データ'をn回繰り返すリスト生成\n",
        "    act = ['正解データ'] * n\n",
        "    pred = ['予測結果'] * n\n",
        "\n",
        "    #データフレーム生成\n",
        "    cm = pd.DataFrame(matrix, \n",
        "        columns=[pred, columns], index=[act, columns])\n",
        "    return cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#デプロイ用のモデルに変更する\n",
        "copy_model_ft = model_ft\n",
        "copy_model_ft.eval()\n",
        "model_ft = repvgg_model_convert(copy_model_ft, create_RepVGG_A2).to(device)\n",
        "\"\"\"\n",
        "\n",
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "targets, preds, probs =[], [], []\n",
        "for image_tensor, target in test_loader:  \n",
        "      #target = target.squeeze(1)     \n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      #_, pred = torch.max(output, 1)  \n",
        "      prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "      \n",
        "      preds.append(int(pred))  #予測結果\n",
        "      targets.append(int(target)) #ラベル\n",
        "\n",
        "      #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "      prob = abs(1-float(prob)-float(pred))\n",
        "      probs.append(prob)  #予測結果(確率)\n",
        "\n",
        "y_test = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "\n",
        "print(y_test)\n",
        "print(y_pred)\n",
        "#print(y_prob)\n",
        "\n",
        "\n",
        "# 混同行列(confusion matrix)の取得\n",
        "labels = class_names\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(matrix)\n",
        "\n",
        "# make_cmを使った混同行列標示\n",
        "cm = make_cm(matrix, labels)\n",
        "\n",
        "# 結果の表示\n",
        "display(cm)\n",
        "\n",
        "\n",
        "tn, fp, fn, tp = matrix.flatten()\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "precision = tp/(tp+fp) #positive predictive value\n",
        "sensitivity = tp/(tp+fn)\n",
        "specificity = tn/(tn+fp)\n",
        "f1_score = 2*tp/(2*tp+fp+fn)\n",
        "\n",
        "#感度・特異度の計算\n",
        "print()\n",
        "print('accuracy: '+str(accuracy))\n",
        "print('positive predictive value: '+str(precision))\n",
        "print('sensitivity: '+str(sensitivity))\n",
        "print('specificity: '+str(specificity))\n",
        "print('f1_score: '+str(f1_score))\n",
        "print()\n",
        "\n",
        "\n",
        "fpr, tpr, thres = metrics.roc_curve(y_test, y_prob)\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "print('auc:', auc)\n",
        "\n",
        "#Draw AUC curve\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 1 0 5 4 5 5 0 1 5 2 3 5 4 4 0 5 0 1 5 5 3 0 5 0 3 0 0 5 0 1 5 1 1 0 1 5\n",
            " 0 1 3 5 0 0 3 4 0 1 0 5 1 0 0 5 0 5 4 1 1 0 1 0 0 1 0 0 0 5 0 0 4 2 4 0 5\n",
            " 5 0 1 1 0 0 0 5 3 5 0]\n",
            "[0 1 0 5 0 5 5 0 1 5 0 3 5 0 0 0 5 0 1 5 2 3 0 5 0 3 0 1 5 0 1 5 1 1 0 1 0\n",
            " 0 1 3 5 0 0 3 4 0 3 0 5 1 0 5 5 0 5 0 1 1 0 0 0 0 1 0 0 0 5 0 0 4 0 0 0 5\n",
            " 5 4 1 1 0 0 0 5 0 5 0]\n",
            "[[29  1  0  0  1  1]\n",
            " [ 1 14  0  1  0  0]\n",
            " [ 2  0  0  0  0  0]\n",
            " [ 1  0  0  5  0  0]\n",
            " [ 6  0  0  0  2  0]\n",
            " [ 1  0  1  0  0 19]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       予測結果                \\\n",
              "                             corrected_cont corrected_eso   \n",
              "正解データ corrected_cont                     29             1   \n",
              "      corrected_eso                       1            14   \n",
              "      corrected_eso-boundary              2             0   \n",
              "      corrected_exo                       1             0   \n",
              "      corrected_exo-boundary              6             0   \n",
              "      corrected_inadequate                1             0   \n",
              "\n",
              "                                                                   \\\n",
              "                             corrected_eso-boundary corrected_exo   \n",
              "正解データ corrected_cont                              0             0   \n",
              "      corrected_eso                               0             1   \n",
              "      corrected_eso-boundary                      0             0   \n",
              "      corrected_exo                               0             5   \n",
              "      corrected_exo-boundary                      0             0   \n",
              "      corrected_inadequate                        1             0   \n",
              "\n",
              "                                                                          \n",
              "                             corrected_exo-boundary corrected_inadequate  \n",
              "正解データ corrected_cont                              1                    1  \n",
              "      corrected_eso                               0                    0  \n",
              "      corrected_eso-boundary                      0                    0  \n",
              "      corrected_exo                               0                    0  \n",
              "      corrected_exo-boundary                      2                    0  \n",
              "      corrected_inadequate                        0                   19  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"6\" halign=\"left\">予測結果</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>corrected_cont</th>\n",
              "      <th>corrected_eso</th>\n",
              "      <th>corrected_eso-boundary</th>\n",
              "      <th>corrected_exo</th>\n",
              "      <th>corrected_exo-boundary</th>\n",
              "      <th>corrected_inadequate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">正解データ</th>\n",
              "      <th>corrected_cont</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corrected_eso</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corrected_eso-boundary</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corrected_exo</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corrected_exo-boundary</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corrected_inadequate</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-ac0b9fef9f51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#positive predictive value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2YAAHbdOJbv"
      },
      "source": [
        "#**GradCAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSDG3Lt1OMS_"
      },
      "source": [
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "# Split model in two parts\n",
        "features_fn = nn.Sequential(*list(model_ft.children())[:-2]) #最後の2層（AdaptiveAvgPool2dとLinear)を取り除いたもの\n",
        "classifier_fn = nn.Sequential(*(list(model_ft.children())[-2:-1] + [Flatten()] + list(model_ft.children())[-1:])) #最終層の前にFlatten()を挿入\n",
        " #最後の2層\n",
        "\n",
        "#評価モードにする    \n",
        "model_ft = model_ft.eval()\n",
        "model_ft = model_ft.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2c8FBsHOMW1"
      },
      "source": [
        "def GradCAM(img, c, features_fn, classifier_fn):\n",
        "    feats = features_fn(img.cuda())\n",
        "    _, N, H, W = feats.size() #[1,2048,7,7]\n",
        "    out = classifier_fn(feats) #out: [1,1000]\n",
        "    c_score = out[0, c]   #c_scoreとは？？\n",
        "\n",
        "    grads = torch.autograd.grad(c_score, feats)\n",
        "    w = grads[0][0].mean(-1).mean(-1)           #ここでGlobalAveragePoolingをしている\n",
        "    sal = torch.matmul(w, feats.view(N, H*W))\n",
        "    sal = sal.view(H, W).cpu().detach().numpy()\n",
        "    sal = np.maximum(sal, 0) #ReLUと同じ\n",
        "    return sal\n",
        "\n",
        "read_tensor = transforms.Compose([\n",
        "    lambda x: Image.open(x),\n",
        "    lambda x: x.convert('RGB'),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    lambda x: torch.unsqueeze(x, 0) #次元を1に引き延ばす\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEC41LROViv"
      },
      "source": [
        "#画像のパスを指定\n",
        "#for j in range(1):\n",
        "for j in range(len(test_dataset)):\n",
        "\n",
        "    #元画像\n",
        "\n",
        "    image = test_dataset[j][0]\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    img_tensor = test_dataset[j][0].unsqueeze(0)\n",
        "    #Softmaxにかけたときの確率上位1つのpp(確率)とcc(class番号)を取得(tench→正常,goldfish→斜視)\n",
        "    pp, cc = torch.topk(nn.Softmax(dim=1)(model_ft(img_tensor.to(device))), 1)\n",
        "\n",
        "    #pとcを対にして入力\n",
        "    for i, (p, c) in enumerate(zip(pp[0], cc[0])):  \n",
        "        sal = GradCAM(img_tensor, int(c), features_fn, classifier_fn)\n",
        "        tmp = image.to('cpu').detach().numpy().copy()\n",
        "        img = Image.fromarray((tmp*255).astype(np.uint8))\n",
        "        #TensorをImageに変換\n",
        "        sal = Image.fromarray(sal)\n",
        "        sal = sal.resize(img.size, resample=Image.LINEAR)\n",
        "\n",
        "        print()\n",
        "        #print(img_path) #あとで参照しやすいように画像のパスを表示\n",
        "\n",
        "        #plt.title('')\n",
        "        print('label: '+labels[test_dataset[j][1]])\n",
        "        print('pred:  '+'{}  {:.1f}%'.format(labels[c], 100*float(p)))\n",
        "        #plt.title('pred:'+'{}: { .1f}%'.format(labels[c], 100*float(p)))        \n",
        "        \n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        #グラフを1行2列に並べたうちの1番目\n",
        "        plt.subplots_adjust(wspace=0,hspace=0)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(np.array(sal), alpha=0.5, cmap='jet')\n",
        "\n",
        "        #元の画像を並べて表示\n",
        "        image = test_dataset[j][0]\n",
        "        image = image.permute(1, 2, 0)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF"
      },
      "source": [
        "#**新しいデータセットの判定**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = r\"F:\\Strabismus\\Dataset_101to200\"\n",
        "dst_path_parent = r\"F:\\Strabismus\\Dataset_101to200_eval\"\n",
        "whole_dataset_dir = r\"C:\\Users\\ykita\\OneDrive\\デスクトップ\\眼位写真NEW\" "
      ],
      "metadata": {
        "id": "TyWB7lRV8vKZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_num = 101\n",
        "end_num = 200\n",
        "\n",
        "#パス冒頭の番号が上の条件を満たすものをリスト化する\n",
        "path_list = [i for i in os.listdir(whole_dataset_dir) if int(i.split(\"-\")[0]) >= start_num and int(i.split(\"-\")[0]) <= end_num]\n",
        "\n",
        "#該当する画像をdata_dirにコピー\n",
        "for i in path_list:\n",
        "    if os.path.splitext(i) == \"jpg\" or \"jpeg\" or \"JPG\" or \"JPEG\":\n",
        "        shutil.copy(os.path.join(whole_dataset_dir,i), data_dir)\n",
        "        print(str(i))\n"
      ],
      "metadata": {
        "id": "sqq5ZsFY8w2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft.to(device)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_names = [\"cont\", \"eso\", \"eso-boundary\", \"exo\", \"exo_boundary\", \"inadequate\"]\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#valフォルダ内のファイル名を取得\n",
        "image_path = glob.glob(data_dir + \"/*\")\n",
        "#random.shuffle(image_path)  #表示順をランダムにする\n",
        "print('number of images: ' +str(len(image_path)))\n",
        "#print(image_path) \n",
        "\n",
        "\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    #expand2square\n",
        "    new_image = expand2square(image,(0,0,0))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(new_image)\n",
        "    #show_image(image_tensor)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, class_names):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "\n",
        "def show_image(image_path):\n",
        "    \"\"\"\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2.imshow(\"title\", resized_img)\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    width, height = img.size\n",
        "    resized_img = img.resize((int(width*300/height), 300))\n",
        "    #画像をarrayに変換\n",
        "    im_list = np.asarray(resized_img)\n",
        "    #貼り付け\n",
        "    plt.imshow(im_list)\n",
        "    #表示\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#ここからがメイン\n",
        "for i in image_path:\n",
        "    print('Image: '+ str(i))\n",
        "    show_image(i) #画像を表示\n",
        "    image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "    model_pred, prob, pred = image_eval(image_tensor, class_names)  #予測結果を出力   \n",
        "    print('Pred: '+ model_pred)\n",
        "    print() #空白行を入れる\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    #ファイルを判定結果に従ってフォルダ分けする\n",
        "    dst_name = os.path.basename(i)\n",
        "    shutil.copy(i, os.path.join(dst_path_parent, class_names[pred], os.path.basename(i)))"
      ],
      "metadata": {
        "id": "yYDu2D7Fgekn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_folder = [r\"F:\\Strabismus\\Dataset_1to100_eval\", r\"F:\\Strabismus\\Dataset_101to200_eval\"]\n",
        "cont_list, eso_list, eso_boundary_list, exo_list, exo_boundary_list, inadequate_list = [],[],[],[],[],[]\n",
        "\n",
        "def addlist(pathlist, classlist):\n",
        "    for i in pathlist:\n",
        "        classlist.append(i)\n",
        "    return classlist\n",
        "\n",
        "for folder in parent_folder:\n",
        "    #cont\n",
        "    addlist(glob.glob(folder+\"/cont/*\"), cont_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_cont/*\"), cont_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_cont/*\"), cont_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_boundary_cont/*\"), cont_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_boundary_cont/*\"), cont_list)\n",
        "    addlist(glob.glob(folder+\"/*/inadequate_cont/*\"), cont_list)\n",
        "    print(\"cont: \", len(cont_list))\n",
        "    #eso\n",
        "    addlist(glob.glob(folder+\"/eso/*\"), eso_list)\n",
        "    addlist(glob.glob(folder+\"/*/cont_eso/*\"), eso_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_boundary_eso/*\"), eso_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_boundary_eso/*\"), eso_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_eso\"), eso_list)\n",
        "    addlist(glob.glob(folder+\"/*/inadequate_eso/*\"), eso_list)\n",
        "    print(\"eso: \", len(eso_list))\n",
        "    #eso_boundary\n",
        "    addlist(glob.glob(folder+\"/eso_boundary/*\"), eso_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/cont_eso_boundary/*\"), eso_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_eso_boundary/*\"), eso_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_boundary_eso_boundary/*\"), eso_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_eso_boundary/*\"), eso_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/inadequate_eso_boundary/*\"), eso_boundary_list)\n",
        "    print(\"eso_boundary: \", len(eso_boundary_list))\n",
        "    #exo\n",
        "    addlist(glob.glob(folder+\"/exo/*\"), exo_list)\n",
        "    addlist(glob.glob(folder+\"/*/cont_exo/*\"), exo_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_boundary_exo/*\"), exo_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_boundary_exo/*\"), exo_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_exo/*\"), exo_list)\n",
        "    addlist(glob.glob(folder+\"/*/inadequate_exo/*\"), exo_list)\n",
        "    print(\"exo: \", len(exo_list))\n",
        "    #exo_boundary\n",
        "    addlist(glob.glob(folder+\"/exo_boundary/*\"), exo_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/cont_exo_boundary/*\"), exo_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_exo_boundary/*\"), exo_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_boundary_exo_boundary/*\"), exo_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_exo_boundary/*\"), exo_boundary_list)\n",
        "    addlist(glob.glob(folder+\"/*/inadequate_exo_boundary/*\"), exo_boundary_list)\n",
        "    print(\"exo_boundary: \", len(exo_boundary_list))\n",
        "    #inadequate\n",
        "    addlist(glob.glob(folder+\"/inadequate/*\"), inadequate_list)\n",
        "    addlist(glob.glob(folder+\"/*/cont_inadequate/*\"), inadequate_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_inadequate/*\"), inadequate_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_boundary_inadequate/*\"), inadequate_list)\n",
        "    addlist(glob.glob(folder+\"/*/eso_inadequate/*\"), inadequate_list)\n",
        "    addlist(glob.glob(folder+\"/*/exo_boundary_inadequate/*\"), inadequate_list)\n",
        "    print(\"inadequate: \",len(inadequate_list))\n",
        "    print(\"\")\n",
        "\n",
        "#print(len(cont_list))\n",
        "\n",
        "#print(len(eso_list),len(eso_boundary_list), len(cont_list), len(exo_list), len(exo_boundary_list), len(inadequate_list))"
      ],
      "metadata": {
        "id": "cc-O10kdJMtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5575f0-714f-4276-bae7-54bc419dafa5"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cont:  357\n",
            "eso:  123\n",
            "eso_boundary:  21\n",
            "exo:  42\n",
            "exo_boundary:  58\n",
            "inadequate:  217\n",
            "\n",
            "cont:  670\n",
            "eso:  147\n",
            "eso_boundary:  32\n",
            "exo:  135\n",
            "exo_boundary:  89\n",
            "inadequate:  566\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_HwRa0nNbmpf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}