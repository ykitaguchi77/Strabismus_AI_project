{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled80.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOJD6oQStKOWu92SZoWGrmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Strabismus_AI_project/blob/main/5-fold%20crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5CVPFZ3z0ON"
      },
      "source": [
        "#**Strabismus 5-fold crossvalidation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ropSQkzzmq"
      },
      "source": [
        "\"\"\"\n",
        "CSVを作成し、ファイル名-正解-予測を記載\n",
        "\n",
        "データを5分割（division1~5)\n",
        "set1: 1がtest、2~5がtrain\n",
        "set2: 2がtest、1、3~5がtrain\n",
        "set3:\n",
        "set4: \n",
        "set5: 5がtest、1~4がtrain\n",
        "それぞれのdivisionに、exoとcontのフォルダを置いておく\n",
        "\n",
        "各セットについてトレーニング→CSVに予測の結果を記載していく\n",
        "全てのデータが揃ったら、正解-予測のデータから、正解率、感度、特異度、ROC curveの計算を行う\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyW4AJj14_og"
      },
      "source": [
        "#**Split dataset for Crossvalidation**\n",
        "trainセットを５分割、うち1つをvalセット、残りの合計をtestセットに分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "PnyR-OC25V0Q",
        "outputId": "a222a805-bf02-41d8-e598-84ee6399e753"
      },
      "source": [
        "import random\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "'''\n",
        "-----orig_data-----grav\n",
        "                |--cont\n",
        "↓\n",
        "↓\n",
        "\n",
        "-----dst_data[0]------dst_train[0]----grav\n",
        "  |                |               |-- cont\n",
        "  |                |--dst_val[0]------grav\n",
        "  |                                |--cont\n",
        "  |\n",
        "  |--dst_data[1]------dst_train[1]----grav\n",
        "  |                |               |-- cont\n",
        "  |                |--dst_val[1]------grav\n",
        "  |                                |--cont\n",
        "  ...\n",
        "  |--dst_data[1]------dst_train[9]----grav\n",
        "                   |               |-- cont\n",
        "                   |--dst_val[9]------grav\n",
        "                                   |--cont\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n-----orig_data-----grav\\n                |--cont\\n↓\\n↓\\n\\n-----dst_data[0]------dst_train[0]----grav\\n  |                |               |-- cont\\n  |                |--dst_val[0]------grav\\n  |                                |--cont\\n  |\\n  |--dst_data[1]------dst_train[1]----grav\\n  |                |               |-- cont\\n  |                |--dst_val[1]------grav\\n  |                                |--cont\\n  ...\\n  |--dst_data[1]------dst_train[9]----grav\\n                   |               |-- cont\\n                   |--dst_val[9]------grav\\n                                   |--cont\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-XraZlu5a5d"
      },
      "source": [
        "def get_path(orig_path, dst_path, split_num):\n",
        "    classes = os.listdir(orig_path) #クラス名を取得\n",
        "    #データの分割数を設定\n",
        "    data_list = [0]*len(classes)\n",
        "    k=0\n",
        "    for i in classes:\n",
        "        data_list[k] = glob.glob(orig_path+'/'+i+'/*')\n",
        "        k+=1\n",
        "    split_length = int(len(data_list)/split_num)\n",
        "    return data_list, classes, split_length\n",
        "\n",
        "def makefolder(orig_path, dst_path, classes):\n",
        "    #フォルダを作成\n",
        "    if os.path.exists(dst_path):\n",
        "        shutil.rmtree(dst_path)\n",
        "        print(\"Existing dataset deleted.\")\n",
        "    elif not os.path.exists(dst_path):  # ディレクトリがなかったら\n",
        "        os.mkdir(dst_path)  # 作成したいフォルダ名を作成\n",
        "        for i in range(split_num):\n",
        "            os.mkdir(dst_path+'/'+str(i))\n",
        "            os.mkdir(dst_path+'/'+str(i)+'/train')\n",
        "            os.mkdir(dst_path+'/'+str(i)+'/val')\n",
        "            for j in classes:\n",
        "                os.mkdir(dst_path+'/'+str(i)+'/train/'+j)\n",
        "                os.mkdir(dst_path+'/'+str(i)+'/val/'+j)\n",
        "\n",
        "def split_data_list(data_list, split_num):\n",
        "    split_data, dst_data, dst_train, dst_val, dst_test = [0]*split_num, [0]*split_num, [0]*split_num, [0]*split_num, [0]*split_num\n",
        "\n",
        "    #データの分割\n",
        "    split_data = list(np.array_split(data_list, split_num))\n",
        "\n",
        "    #データセット全体と分割したデータの差分を取り、dst_dataに格納\n",
        "\n",
        "    dst_data = [0] * split_num\n",
        "    for i in range(split_num):\n",
        "        dst_data[i] = [x for x in data_list if x not in split_data[i]]\n",
        "\n",
        "    #トレーニングセット、バリデーションセット、テストセットのリスト作成\n",
        "    for i in range(split_num):\n",
        "        dst_train[i] = dst_data[i]\n",
        "        dst_val[i] = split_data[i]  #テストセット\n",
        "    \n",
        "    return dst_train, dst_val\n",
        "\n",
        "def copy_to_folders(split_num, class_name, dst_train, dst_val, dst_path):\n",
        "    k=0\n",
        "    for i in range(split_num):\n",
        "        dst_path_train = dst_path +'/'+str(i)+'/train/'+class_name\n",
        "        dst_path_val = dst_path +'/'+str(i)+'/val/'+class_name\n",
        "        for p in dst_train[k]:  # 選択したファイルを目的フォルダにコピー\n",
        "            shutil.copy(p, dst_path_train)\n",
        "            #print(p)\n",
        "            print(dst_path_train)\n",
        "\n",
        "        for p in dst_val[k]:  # 選択したファイルを目的フォルダにコピー\n",
        "            shutil.copy(p, dst_path_val)\n",
        "            #print(p)    \n",
        "            print(dst_path_val)\n",
        "\n",
        "        k+=1    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmDnaPiLHxm4"
      },
      "source": [
        "#パスの設定\n",
        "orig_path = \"/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_20211111\"\n",
        "dst_path = \"/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo\"  # フォルダ名\n",
        "csv_path = dst_path + \"/img_list.csv\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIxM04cGIfQo",
        "outputId": "fd44c209-7f24-419a-cf48-0ae8e00d9664"
      },
      "source": [
        "#Dataの分割の設定\n",
        "split_num = 5  #データをいくつに分割するかを記載\n",
        "data_list, classes, split_length = get_path(orig_path, dst_path, split_num)\n",
        "print(classes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cont', 'eso', 'exo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnfDmSsjI8yX"
      },
      "source": [
        "#作成するデータのクラスを設定し直す\n",
        "#疾患群を後ろにする\n",
        "classes = ['cont', 'eso']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHp4Whs25lbn"
      },
      "source": [
        "#5-foldデータセットの作成\n",
        "#作成済みの場合には省略\n",
        "makefolder(orig_path, dst_path, classes)\n",
        "print(classes)\n",
        "k=0\n",
        "for i in range(len(classes)):\n",
        "    dst_train, dst_val = split_data_list(data_list[k], split_num)\n",
        "    print(classes[k])\n",
        "    copy_to_folders(split_num, classes[k], dst_train, dst_val, dst_path)\n",
        "    k+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SlzcdphHIA7"
      },
      "source": [
        "#**Make CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFwa0-VP-fR5"
      },
      "source": [
        "classes = os.listdir(dst_path+\"/0/val/\")\n",
        "split_num = 5\n",
        "img_num = len(glob.glob(dst_path+\"/*/val/*/*\"))\n",
        "#print(glob.glob(dst_path+\"/*/val/*/*\"))\n",
        "#print(img_num)\n",
        "\n",
        "#Make CSV file\n",
        "cols = [\"num\", \"division\", \"class\", \"prediction\"]\n",
        "df_cross = pd.DataFrame(index=list(range(img_num)), columns=cols)\n",
        "\n",
        "t=0\n",
        "for i in range(split_num):\n",
        "    for j in classes:\n",
        "        #print(str(i) + \", \"+str(j))\n",
        "        img_list = os.listdir(dst_path+\"/\"+str(i)+\"/val/\"+str(j))\n",
        "        #print(img_list)\n",
        "        for k in img_list:\n",
        "            df_cross.iloc[t,0]=str(k)\n",
        "            df_cross.iloc[t,1]=str(i)\n",
        "            df_cross.iloc[t,2]=str(j)\n",
        "            t+=1\n",
        "df_cross"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXnQMFDKL7gF"
      },
      "source": [
        "#CSV fileを保存\n",
        "csv = df_cross.to_csv(csv_path, encoding='utf_8_sig')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRZtvJGoHZIp"
      },
      "source": [
        "#**Training & evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZCjIXc4ULq1",
        "outputId": "13c6d314-d731-4f62-a3f3-87e04381e5f1"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (3.10.0.2)\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiozE-KzUZPp"
      },
      "source": [
        "#Module群\n",
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob, model_pred_class, image_path\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n",
        "\n",
        "def makefolder(path):\n",
        "    if not os.path.exists(path):  # ディレクトリがなかったら\n",
        "        os.mkdir(path)  # 作成したいフォルダ名を作成\n",
        "\n",
        "def prediction_results_to_df(df, img_path, pred):\n",
        "    for path, pred in zip(img_path, pred):\n",
        "        img_name = os.path.basename(path)\n",
        "        df.loc[df[\"num\"] == img_name, \"prediction\"] = pred\n",
        "\n",
        "def convnet():\n",
        "    model_ft = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ugZyB_gxU2uX",
        "outputId": "31a9df1f-74e1-45b0-c30e-35b7653f451a"
      },
      "source": [
        "#まとめて解析\n",
        "# 出力名を記入\n",
        "out_name = \"ResNet50_pretrained_all\"\n",
        "csv_path = dst_path + \"/img_list_\"+out_name+\".csv\"\n",
        "\n",
        "#create data_dir_list\n",
        "data_dir = dst_path\n",
        "fold = split_num\n",
        "print(str(fold)+'-fold cross validation')\n",
        "\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "#print(roc_label_list)\n",
        "\n",
        "\n",
        "\n",
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "#print(data_dir_list)\n",
        "#print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    torch.save(model_ft.state_dict(), data_dir + '/'+str(out_name)+\"_\"+str(i)+\".pth\")    #ネットワークの保存\n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob, model_pred_class, val_img_path = evaluation(model_ft, data_dir + \"/\" +str(i)+\"/val\")\n",
        "    prediction_results_to_df(df_cross, val_img_path, model_pred_class)\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob, class_names)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list), class_names)\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# CSVとして出力\n",
        "makefolder(data_dir + \"/crossvalidation_csv\")\n",
        "df.to_csv(data_dir + \"/crossvalidation_csv/\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "makefolder(data_dir + \"/crossvalidation_ROCfigure\")\n",
        "fig.savefig(data_dir + \"/crossvalidation_ROCfigure/\" + out_name +\".png\")\n",
        "\n",
        "#Save ROC data\n",
        "makefolder(data_dir + \"/crossvalidation_ROCdata\")\n",
        "with open(data_dir + \"/crossvalidation_ROCdata/\"+out_name+\".csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n",
        "#リストCSVを保存\n",
        "df_cross.to_csv(csv_path, encoding='utf_8_sig')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-fold cross validation\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/0\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/1\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/2\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/3\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/4\n",
            "['cont', 'exo']\n",
            "cont_train:292\n",
            "exo_train:238\n",
            "cont_val:74\n",
            "exo_val:60\n",
            "training data set_total：530\n",
            "validating data set_total：134\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.9935 Acc: 0.5547\n",
            "val Loss: 1.9129 Acc: 0.5224\n",
            "Validation loss decreased (inf --> 1.912889).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.6569 Acc: 0.6396\n",
            "val Loss: 1.5131 Acc: 0.4179\n",
            "Validation loss decreased (1.912889 --> 1.513125).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7636 Acc: 0.5849\n",
            "val Loss: 1.0412 Acc: 0.3955\n",
            "Validation loss decreased (1.513125 --> 1.041247).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.7766 Acc: 0.5906\n",
            "val Loss: 1.1187 Acc: 0.4701\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7029 Acc: 0.6170\n",
            "val Loss: 4.1989 Acc: 0.4030\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.6989 Acc: 0.5887\n",
            "val Loss: 0.8715 Acc: 0.3881\n",
            "Validation loss decreased (1.041247 --> 0.871455).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.7223 Acc: 0.6245\n",
            "val Loss: 1.3440 Acc: 0.4478\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7663 Acc: 0.6075\n",
            "val Loss: 1.1793 Acc: 0.5224\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.7537 Acc: 0.5925\n",
            "val Loss: 0.8223 Acc: 0.5000\n",
            "Validation loss decreased (0.871455 --> 0.822279).  Saving model ...\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.9025 Acc: 0.5472\n",
            "val Loss: 0.9767 Acc: 0.4328\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7280 Acc: 0.6094\n",
            "val Loss: 4.7903 Acc: 0.4627\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7043 Acc: 0.6208\n",
            "val Loss: 26.3310 Acc: 0.5597\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7893 Acc: 0.6566\n",
            "val Loss: 1.2071 Acc: 0.4851\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.8505 Acc: 0.5962\n",
            "val Loss: 0.8071 Acc: 0.5299\n",
            "Validation loss decreased (0.822279 --> 0.807133).  Saving model ...\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7354 Acc: 0.5547\n",
            "val Loss: 1.0253 Acc: 0.4627\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.7369 Acc: 0.5981\n",
            "val Loss: 0.7032 Acc: 0.5224\n",
            "Validation loss decreased (0.807133 --> 0.703171).  Saving model ...\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.7442 Acc: 0.5943\n",
            "val Loss: 0.9196 Acc: 0.6045\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7572 Acc: 0.6358\n",
            "val Loss: 1.3793 Acc: 0.4478\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.7261 Acc: 0.6264\n",
            "val Loss: 2.4110 Acc: 0.5746\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.7242 Acc: 0.6264\n",
            "val Loss: 0.9123 Acc: 0.4552\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.6637 Acc: 0.6302\n",
            "val Loss: 0.7597 Acc: 0.4328\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.6722 Acc: 0.6170\n",
            "val Loss: 1.4631 Acc: 0.5224\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.6941 Acc: 0.6302\n",
            "val Loss: 104.8914 Acc: 0.4925\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.7587 Acc: 0.6208\n",
            "val Loss: 0.7825 Acc: 0.4403\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.6644 Acc: 0.6208\n",
            "val Loss: 0.7602 Acc: 0.4179\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.6759 Acc: 0.6434\n",
            "val Loss: 0.9939 Acc: 0.5075\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.6847 Acc: 0.6075\n",
            "val Loss: 16.3413 Acc: 0.4701\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.7277 Acc: 0.6321\n",
            "val Loss: 3.9527 Acc: 0.5373\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.6685 Acc: 0.6075\n",
            "val Loss: 7.1585 Acc: 0.4403\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.6635 Acc: 0.6642\n",
            "val Loss: 0.8523 Acc: 0.4627\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.5909 Acc: 0.6736\n",
            "val Loss: 5.2674 Acc: 0.4403\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 2m 11s\n",
            "Best val Acc: 0.604478\n",
            "number of images: 134\n",
            "14 46 61 13\n",
            "Accuracy: 0.5597014925373134\n",
            "Precision (positive predictive value): 0.5185185185185185\n",
            "Recall (sensitivity): 0.23333333333333334\n",
            "Specificity: 0.8243243243243243\n",
            "F_value: 0.32183908045977017\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "134\n",
            "[0.09006243944168091, 0.10071557760238647, 0.11443597078323364, 0.07955318689346313, 0.08072316646575928, 0.21091943979263306, 0.3842979669570923, 0.11385506391525269, 0.3362409472465515, 0.08899253606796265, 0.10462957620620728, 0.09060746431350708, 0.08921068906784058, 0.08589053153991699, 0.08346396684646606, 0.08795058727264404, 0.10462307929992676, 0.1622523069381714, 0.17697018384933472, 0.16849994659423828, 0.14421653747558594, 0.20382559299468994, 0.1918819546699524, 0.15303665399551392, 0.15658730268478394, 0.17766284942626953, 0.7304185628890991, 0.4577453136444092, 0.2710447907447815, 0.5132108330726624, 0.2871033549308777, 0.22640621662139893, 0.313477098941803, 0.5645963549613953, 0.294827938079834, 0.6040617823600769, 0.5143424272537231, 0.2608714699745178, 0.3064981698989868, 0.39871448278427124, 0.5207480788230896, 0.566920816898346, 0.5129740238189697, 0.2286994457244873, 0.3824787139892578, 0.29122310876846313, 0.2363070249557495, 0.2921410799026489, 0.5742039084434509, 0.21659022569656372, 0.30055367946624756, 0.32104891538619995, 0.286598801612854, 0.3300623893737793, 0.27336347103118896, 0.29742956161499023, 0.6320202350616455, 0.6362401843070984, 0.44648003578186035, 0.5573561787605286, 0.2485121488571167, 0.22081440687179565, 0.37746649980545044, 0.4833236336708069, 0.4739077091217041, 0.29308873414993286, 0.15497761964797974, 0.22878921031951904, 0.4123772978782654, 0.19192159175872803, 0.6240944862365723, 0.19611793756484985, 0.3054424524307251, 0.15231025218963623, 0.09954291582107544, 0.10235828161239624, 0.23428916931152344, 0.16306674480438232, 0.7790592908859253, 0.5128095149993896, 0.3848366141319275, 0.08312386274337769, 0.12044161558151245, 0.1007959246635437, 0.09622830152511597, 0.09105461835861206, 0.0847938060760498, 0.5291953086853027, 0.16835874319076538, 0.21234142780303955, 0.22915786504745483, 0.19371908903121948, 0.7352749109268188, 0.27238333225250244, 0.2801669239997864, 0.5770280361175537, 0.08784794807434082, 0.7797135710716248, 0.1406223177909851, 0.48769527673721313, 0.16044825315475464, 0.16186964511871338, 0.33307522535324097, 0.08791667222976685, 0.7250481247901917, 0.08328640460968018, 0.750491201877594, 0.21239715814590454, 0.5807823538780212, 0.11493754386901855, 0.0788419246673584, 0.10257458686828613, 0.34413760900497437, 0.14361929893493652, 0.690924346446991, 0.5299263000488281, 0.07675457000732422, 0.45441383123397827, 0.8193594813346863, 0.08402419090270996, 0.4882850646972656, 0.09567093849182129, 0.08810865879058838, 0.2167649269104004, 0.11184662580490112, 0.24426132440567017, 0.13985604047775269, 0.37360596656799316, 0.10721367597579956, 0.09765452146530151, 0.08598506450653076, 0.6727064847946167, 0.6154530644416809, 0.14214301109313965]\n",
            "134\n",
            "roc_auc: 0.4394144144144144\n",
            "\n",
            "\n",
            "['cont', 'exo']\n",
            "cont_train:293\n",
            "exo_train:238\n",
            "cont_val:73\n",
            "exo_val:60\n",
            "training data set_total：531\n",
            "validating data set_total：133\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.0676 Acc: 0.5066\n",
            "val Loss: 0.8045 Acc: 0.5865\n",
            "Validation loss decreased (inf --> 0.804454).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.6788 Acc: 0.5687\n",
            "val Loss: 0.7082 Acc: 0.5789\n",
            "Validation loss decreased (0.804454 --> 0.708189).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7374 Acc: 0.5782\n",
            "val Loss: 0.7314 Acc: 0.6466\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.6884 Acc: 0.6008\n",
            "val Loss: 0.7052 Acc: 0.6466\n",
            "Validation loss decreased (0.708189 --> 0.705210).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7990 Acc: 0.5819\n",
            "val Loss: 0.6747 Acc: 0.5414\n",
            "Validation loss decreased (0.705210 --> 0.674736).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.7200 Acc: 0.5480\n",
            "val Loss: 0.6451 Acc: 0.6466\n",
            "Validation loss decreased (0.674736 --> 0.645131).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.8555 Acc: 0.5800\n",
            "val Loss: 0.8103 Acc: 0.4361\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7909 Acc: 0.5631\n",
            "val Loss: 0.6663 Acc: 0.6090\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.9303 Acc: 0.5932\n",
            "val Loss: 1.2155 Acc: 0.4737\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.7263 Acc: 0.5951\n",
            "val Loss: 0.6718 Acc: 0.6090\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7003 Acc: 0.6045\n",
            "val Loss: 0.7889 Acc: 0.4361\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7218 Acc: 0.5669\n",
            "val Loss: 0.6955 Acc: 0.5789\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.8041 Acc: 0.6045\n",
            "val Loss: 0.6522 Acc: 0.6316\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.8727 Acc: 0.6008\n",
            "val Loss: 0.7009 Acc: 0.5789\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7565 Acc: 0.5970\n",
            "val Loss: 0.6636 Acc: 0.6316\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.8116 Acc: 0.5932\n",
            "val Loss: 0.6568 Acc: 0.6541\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.7829 Acc: 0.5932\n",
            "val Loss: 0.7100 Acc: 0.5263\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.6691 Acc: 0.6290\n",
            "val Loss: 0.7106 Acc: 0.5940\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.6546 Acc: 0.6196\n",
            "val Loss: 0.6786 Acc: 0.6391\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.6444 Acc: 0.6403\n",
            "val Loss: 0.6585 Acc: 0.6165\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.6643 Acc: 0.6234\n",
            "val Loss: 0.6467 Acc: 0.6015\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 2m 4s\n",
            "Best val Acc: 0.654135\n",
            "number of images: 133\n",
            "32 28 52 21\n",
            "Accuracy: 0.631578947368421\n",
            "Precision (positive predictive value): 0.6037735849056604\n",
            "Recall (sensitivity): 0.5333333333333333\n",
            "Specificity: 0.7123287671232876\n",
            "F_value: 0.5663716814159292\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "133\n",
            "[0.7201749682426453, 0.7768316268920898, 0.5932095646858215, 0.4650750160217285, 0.5214661955833435, 0.4818410873413086, 0.8067145943641663, 0.6910396814346313, 0.6530585289001465, 0.31605392694473267, 0.571061909198761, 0.25155413150787354, 0.6905696392059326, 0.4233347773551941, 0.3110427260398865, 0.4711218476295471, 0.44941699504852295, 0.62582927942276, 0.22220581769943237, 0.3594627380371094, 0.34864991903305054, 0.2052757740020752, 0.3393341898918152, 0.2937510013580322, 0.4904813766479492, 0.2114948034286499, 0.2939034104347229, 0.30294734239578247, 0.5029242634773254, 0.5040267109870911, 0.4889678955078125, 0.2459580898284912, 0.33317315578460693, 0.6750587821006775, 0.4087224006652832, 0.47411394119262695, 0.6203255653381348, 0.350890576839447, 0.23288732767105103, 0.28980541229248047, 0.30459117889404297, 0.26880043745040894, 0.4247285723686218, 0.4247285723686218, 0.5071178078651428, 0.4312165379524231, 0.5507261157035828, 0.2930503487586975, 0.5558497905731201, 0.2875134348869324, 0.40473878383636475, 0.38835370540618896, 0.18081343173980713, 0.41375452280044556, 0.26662391424179077, 0.5157900452613831, 0.5594902038574219, 0.3911092281341553, 0.33657217025756836, 0.30177730321884155, 0.38744115829467773, 0.592248260974884, 0.31976568698883057, 0.25929808616638184, 0.34031325578689575, 0.26171761751174927, 0.31225860118865967, 0.38850247859954834, 0.38454121351242065, 0.337383508682251, 0.31479722261428833, 0.721421480178833, 0.29101431369781494, 0.33306145668029785, 0.3051130175590515, 0.3940157890319824, 0.7058989405632019, 0.6517325043678284, 0.5425778031349182, 0.49555498361587524, 0.7066948413848877, 0.8557315468788147, 0.5590786933898926, 0.3313964009284973, 0.5430763959884644, 0.43754225969314575, 0.45538032054901123, 0.19081664085388184, 0.2591702342033386, 0.1686040759086609, 0.5503260493278503, 0.4654051661491394, 0.3675824999809265, 0.7075961828231812, 0.39256131649017334, 0.5915536284446716, 0.5580820441246033, 0.42111754417419434, 0.3725811839103699, 0.9140729308128357, 0.4769659638404846, 0.8840194344520569, 0.26618921756744385, 0.7254077792167664, 0.6462068557739258, 0.386189341545105, 0.20313966274261475, 0.7795957922935486, 0.5350841283798218, 0.6267330646514893, 0.3234582543373108, 0.6607373952865601, 0.38218772411346436, 0.4895477294921875, 0.5378705263137817, 0.674558162689209, 0.8174565434455872, 0.33556151390075684, 0.5557329654693604, 0.41397154331207275, 0.7542575001716614, 0.43777263164520264, 0.21497416496276855, 0.7188912034034729, 0.6926093697547913, 0.6827695369720459, 0.00016987323760986328, 0.20145869255065918, 0.8654789328575134, 0.5558843016624451, 0.644378662109375, 0.5844458341598511, 0.6425957679748535]\n",
            "133\n",
            "roc_auc: 0.6527397260273973\n",
            "\n",
            "\n",
            "['cont', 'exo']\n",
            "cont_train:293\n",
            "exo_train:238\n",
            "cont_val:73\n",
            "exo_val:60\n",
            "training data set_total：531\n",
            "validating data set_total：133\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.9586 Acc: 0.5424\n",
            "val Loss: 0.9552 Acc: 0.3910\n",
            "Validation loss decreased (inf --> 0.955221).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7598 Acc: 0.5537\n",
            "val Loss: 0.6737 Acc: 0.5414\n",
            "Validation loss decreased (0.955221 --> 0.673700).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7447 Acc: 0.5725\n",
            "val Loss: 0.7252 Acc: 0.5263\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.6984 Acc: 0.6083\n",
            "val Loss: 0.6814 Acc: 0.6391\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.8684 Acc: 0.5386\n",
            "val Loss: 0.8904 Acc: 0.6617\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.6926 Acc: 0.5612\n",
            "val Loss: 0.6520 Acc: 0.6316\n",
            "Validation loss decreased (0.673700 --> 0.652037).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.7024 Acc: 0.6008\n",
            "val Loss: 0.6744 Acc: 0.6015\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7232 Acc: 0.6102\n",
            "val Loss: 0.6623 Acc: 0.6165\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.6401 Acc: 0.6252\n",
            "val Loss: 0.7509 Acc: 0.6090\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.6773 Acc: 0.6215\n",
            "val Loss: 0.6331 Acc: 0.6466\n",
            "Validation loss decreased (0.652037 --> 0.633085).  Saving model ...\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.6796 Acc: 0.5857\n",
            "val Loss: 0.6524 Acc: 0.6541\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7318 Acc: 0.6328\n",
            "val Loss: 0.8280 Acc: 0.5639\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7423 Acc: 0.5631\n",
            "val Loss: 0.6840 Acc: 0.5639\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.8140 Acc: 0.5913\n",
            "val Loss: 0.6885 Acc: 0.6015\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7032 Acc: 0.5913\n",
            "val Loss: 0.6649 Acc: 0.5489\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.7927 Acc: 0.5574\n",
            "val Loss: 0.6852 Acc: 0.5639\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.8292 Acc: 0.5687\n",
            "val Loss: 1.9008 Acc: 0.6466\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7148 Acc: 0.5951\n",
            "val Loss: 11.9313 Acc: 0.5263\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.7630 Acc: 0.6290\n",
            "val Loss: 0.6334 Acc: 0.6391\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.7578 Acc: 0.5932\n",
            "val Loss: 0.6559 Acc: 0.5865\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.7433 Acc: 0.5989\n",
            "val Loss: 3.3030 Acc: 0.6316\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.7405 Acc: 0.5951\n",
            "val Loss: 0.6757 Acc: 0.5940\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.6943 Acc: 0.6196\n",
            "val Loss: 0.6413 Acc: 0.6617\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.6637 Acc: 0.6121\n",
            "val Loss: 0.7461 Acc: 0.5639\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.7449 Acc: 0.6347\n",
            "val Loss: 1.2585 Acc: 0.6090\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 2m 19s\n",
            "Best val Acc: 0.661654\n",
            "number of images: 133\n",
            "31 29 57 16\n",
            "Accuracy: 0.6616541353383458\n",
            "Precision (positive predictive value): 0.6595744680851063\n",
            "Recall (sensitivity): 0.5166666666666667\n",
            "Specificity: 0.7808219178082192\n",
            "F_value: 0.5794392523364487\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "133\n",
            "[0.38727688789367676, 0.37414413690567017, 0.4952852725982666, 0.6759089231491089, 0.49770039319992065, 0.5787113308906555, 0.4131861925125122, 0.4560896158218384, 0.47933119535446167, 0.4191104769706726, 0.4700954556465149, 0.4577375054359436, 0.4329303503036499, 0.5076321363449097, 0.4430006742477417, 0.40454375743865967, 0.40625882148742676, 0.3853580355644226, 0.4571409225463867, 0.4535332918167114, 0.4270445704460144, 0.38831543922424316, 0.40879255533218384, 0.43823760747909546, 0.5361572504043579, 0.48261165618896484, 0.4003371596336365, 0.33300626277923584, 0.4194028377532959, 0.4671223759651184, 0.8255472183227539, 0.3440282940864563, 0.38062334060668945, 0.4403649568557739, 0.45766037702560425, 0.46895724534988403, 0.41526341438293457, 0.5252509117126465, 0.5463496446609497, 0.9658973813056946, 0.473732590675354, 0.3937395215034485, 0.48946142196655273, 0.4501526951789856, 0.5882874131202698, 0.9992780089378357, 0.47288811206817627, 0.9916662573814392, 0.35975080728530884, 0.9993645548820496, 0.5461894273757935, 0.41508185863494873, 0.45020753145217896, 0.444252073764801, 0.5045738816261292, 0.35654574632644653, 0.4746151566505432, 0.43078893423080444, 0.47100192308425903, 0.45830482244491577, 0.4566981792449951, 0.376142680644989, 0.424432635307312, 0.45016181468963623, 0.3632808327674866, 0.35505425930023193, 0.46146631240844727, 0.8379138708114624, 0.4713476896286011, 0.9127377867698669, 0.44853490591049194, 0.4227113127708435, 0.4924183487892151, 0.5502341985702515, 0.44130903482437134, 0.448444128036499, 0.46162575483322144, 0.5199260115623474, 0.9999943971633911, 0.9407969117164612, 0.4871630072593689, 0.48560458421707153, 0.514015257358551, 0.45417487621307373, 0.9998770952224731, 0.9895211458206177, 0.41941219568252563, 0.45813828706741333, 0.5872339010238647, 0.9999889135360718, 0.8725572824478149, 0.505302369594574, 0.36778414249420166, 0.4440246820449829, 0.41959649324417114, 0.9997612833976746, 0.8089054226875305, 0.4737628102302551, 0.3923758864402771, 0.3328363299369812, 0.9989662170410156, 0.4340360164642334, 0.4506189227104187, 0.4774852991104126, 0.5261862277984619, 0.4595955014228821, 0.5435572862625122, 0.5161525011062622, 0.5024781823158264, 0.49539101123809814, 0.4700368046760559, 1.0, 0.5201478004455566, 0.9980785846710205, 0.8471817970275879, 0.9871756434440613, 0.9997937083244324, 0.3782414197921753, 0.997222900390625, 0.4146159291267395, 0.40000176429748535, 0.48848479986190796, 0.9999998807907104, 0.4794507622718811, 0.9997709393501282, 0.5183166861534119, 0.38858717679977417, 0.5443242192268372, 0.3954029679298401, 0.539848804473877, 0.3944637179374695, 0.3520711660385132, 0.9999974966049194]\n",
            "133\n",
            "roc_auc: 0.6687214611872146\n",
            "\n",
            "\n",
            "['cont', 'exo']\n",
            "cont_train:293\n",
            "exo_train:239\n",
            "cont_val:73\n",
            "exo_val:59\n",
            "training data set_total：532\n",
            "validating data set_total：132\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.1144 Acc: 0.5564\n",
            "val Loss: 0.6568 Acc: 0.6212\n",
            "Validation loss decreased (inf --> 0.656817).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7726 Acc: 0.5432\n",
            "val Loss: 0.7052 Acc: 0.5758\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7600 Acc: 0.5301\n",
            "val Loss: 0.6628 Acc: 0.6061\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.7233 Acc: 0.5789\n",
            "val Loss: 0.6405 Acc: 0.6515\n",
            "Validation loss decreased (0.656817 --> 0.640487).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7670 Acc: 0.5808\n",
            "val Loss: 0.6474 Acc: 0.6591\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.7424 Acc: 0.5207\n",
            "val Loss: 0.6792 Acc: 0.5682\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.7465 Acc: 0.5451\n",
            "val Loss: 2.7728 Acc: 0.5530\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7274 Acc: 0.5489\n",
            "val Loss: 0.6685 Acc: 0.5303\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.7459 Acc: 0.5940\n",
            "val Loss: 0.8346 Acc: 0.5530\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.8220 Acc: 0.5395\n",
            "val Loss: 0.6571 Acc: 0.5985\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7375 Acc: 0.5677\n",
            "val Loss: 0.7763 Acc: 0.5833\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.6974 Acc: 0.5677\n",
            "val Loss: 0.6575 Acc: 0.6212\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7414 Acc: 0.5771\n",
            "val Loss: 0.6517 Acc: 0.6288\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.7086 Acc: 0.5959\n",
            "val Loss: 0.6733 Acc: 0.5682\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.8574 Acc: 0.5658\n",
            "val Loss: 0.7239 Acc: 0.5985\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.8174 Acc: 0.5564\n",
            "val Loss: 0.6297 Acc: 0.6439\n",
            "Validation loss decreased (0.640487 --> 0.629725).  Saving model ...\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.8253 Acc: 0.5056\n",
            "val Loss: 0.6900 Acc: 0.5530\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7951 Acc: 0.5395\n",
            "val Loss: 0.6212 Acc: 0.6667\n",
            "Validation loss decreased (0.629725 --> 0.621212).  Saving model ...\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.7899 Acc: 0.5771\n",
            "val Loss: 0.5962 Acc: 0.6742\n",
            "Validation loss decreased (0.621212 --> 0.596211).  Saving model ...\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.8122 Acc: 0.5658\n",
            "val Loss: 0.6188 Acc: 0.6364\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.8076 Acc: 0.5639\n",
            "val Loss: 0.6691 Acc: 0.6212\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.6881 Acc: 0.6241\n",
            "val Loss: 0.6593 Acc: 0.6212\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.7240 Acc: 0.5827\n",
            "val Loss: 0.6441 Acc: 0.6667\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.6650 Acc: 0.6278\n",
            "val Loss: 0.6513 Acc: 0.6212\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.7084 Acc: 0.6391\n",
            "val Loss: 2.0264 Acc: 0.5682\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.6601 Acc: 0.6241\n",
            "val Loss: 0.6686 Acc: 0.6667\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.6439 Acc: 0.6523\n",
            "val Loss: 0.6667 Acc: 0.6212\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.6752 Acc: 0.6560\n",
            "val Loss: 0.6928 Acc: 0.5455\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.6561 Acc: 0.6128\n",
            "val Loss: 0.6216 Acc: 0.6288\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.6207 Acc: 0.6729\n",
            "val Loss: 0.6091 Acc: 0.6667\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.6823 Acc: 0.6165\n",
            "val Loss: 0.7000 Acc: 0.6136\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.6630 Acc: 0.6504\n",
            "val Loss: 0.6126 Acc: 0.6591\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.6488 Acc: 0.6729\n",
            "val Loss: 0.6538 Acc: 0.6894\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.6175 Acc: 0.6823\n",
            "val Loss: 0.5869 Acc: 0.7121\n",
            "Validation loss decreased (0.596211 --> 0.586875).  Saving model ...\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.6049 Acc: 0.6842\n",
            "val Loss: 0.6226 Acc: 0.6742\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.5753 Acc: 0.6955\n",
            "val Loss: 0.6303 Acc: 0.6439\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.5911 Acc: 0.7086\n",
            "val Loss: 0.5902 Acc: 0.7197\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.5605 Acc: 0.7068\n",
            "val Loss: 0.5922 Acc: 0.6591\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.5426 Acc: 0.7237\n",
            "val Loss: 0.6655 Acc: 0.6515\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.6043 Acc: 0.7237\n",
            "val Loss: 0.9956 Acc: 0.6212\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.6689 Acc: 0.6861\n",
            "val Loss: 0.5843 Acc: 0.6818\n",
            "Validation loss decreased (0.586875 --> 0.584274).  Saving model ...\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.5138 Acc: 0.7632\n",
            "val Loss: 0.5288 Acc: 0.7576\n",
            "Validation loss decreased (0.584274 --> 0.528800).  Saving model ...\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.5564 Acc: 0.7162\n",
            "val Loss: 0.5803 Acc: 0.7197\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.5308 Acc: 0.7519\n",
            "val Loss: 0.9125 Acc: 0.6515\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.4901 Acc: 0.7669\n",
            "val Loss: 0.9225 Acc: 0.6364\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.5112 Acc: 0.7707\n",
            "val Loss: 0.5005 Acc: 0.7576\n",
            "Validation loss decreased (0.528800 --> 0.500475).  Saving model ...\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.4868 Acc: 0.7650\n",
            "val Loss: 0.5318 Acc: 0.7500\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.5401 Acc: 0.7387\n",
            "val Loss: 0.5318 Acc: 0.7045\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.4755 Acc: 0.7820\n",
            "val Loss: 0.6163 Acc: 0.6970\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.4036 Acc: 0.8365\n",
            "val Loss: 0.5088 Acc: 0.7652\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Training complete in 4m 8s\n",
            "Best val Acc: 0.765152\n",
            "number of images: 132\n",
            "35 24 62 11\n",
            "Accuracy: 0.7348484848484849\n",
            "Precision (positive predictive value): 0.7608695652173914\n",
            "Recall (sensitivity): 0.5932203389830508\n",
            "Specificity: 0.8493150684931506\n",
            "F_value: 0.6666666666666667\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "132\n",
            "[0.08310103416442871, 0.11002451181411743, 0.15697813034057617, 0.04422801733016968, 0.14943009614944458, 0.10214865207672119, 0.05019021034240723, 0.31412672996520996, 0.25614213943481445, 0.16796565055847168, 0.11252301931381226, 0.10139131546020508, 0.824539840221405, 0.3180643916130066, 0.7893756031990051, 0.14338642358779907, 0.7165743112564087, 0.024253249168395996, 0.08171188831329346, 0.28546661138534546, 0.29035359621047974, 0.15108895301818848, 0.07658892869949341, 0.44908636808395386, 0.6834841370582581, 0.02402198314666748, 0.25936615467071533, 0.08846122026443481, 0.32756274938583374, 0.038271546363830566, 0.2774394750595093, 0.028152883052825928, 0.06077629327774048, 0.24299168586730957, 0.5095449090003967, 0.07585024833679199, 0.02350449562072754, 0.40947210788726807, 0.09428983926773071, 0.10270321369171143, 0.09959757328033447, 0.08898192644119263, 0.03789311647415161, 0.11826294660568237, 0.12911617755889893, 0.10896658897399902, 0.01841115951538086, 0.18155258893966675, 0.09900999069213867, 0.13793975114822388, 0.31473344564437866, 0.056733012199401855, 0.5236878991127014, 0.1661738157272339, 0.07886278629302979, 0.030424416065216064, 0.41377294063568115, 0.06321394443511963, 0.06375688314437866, 0.05614960193634033, 0.09077531099319458, 0.2625120282173157, 0.4686134457588196, 0.5997464656829834, 0.27931272983551025, 0.23702967166900635, 0.6129030585289001, 0.6774317026138306, 0.06885415315628052, 0.6960786581039429, 0.27008575201034546, 0.9002028703689575, 0.04067647457122803, 0.835390567779541, 0.9445277452468872, 0.764220118522644, 0.9520102739334106, 0.19240766763687134, 0.3334246277809143, 0.8020559549331665, 0.8378894925117493, 0.8961179852485657, 0.7225562930107117, 0.9720748066902161, 0.3908821940422058, 0.9795771837234497, 0.7815772891044617, 0.4949255585670471, 0.5240240097045898, 0.07523846626281738, 0.4804791808128357, 0.10946416854858398, 0.15190017223358154, 0.134801983833313, 0.38276416063308716, 0.07245945930480957, 0.8123735189437866, 0.5572831034660339, 0.34570497274398804, 0.9236764907836914, 0.5971484780311584, 0.2552903890609741, 0.4027189016342163, 0.7043775320053101, 0.6722303032875061, 0.9617714285850525, 0.21962779760360718, 0.9726104140281677, 0.42656999826431274, 0.7188863158226013, 0.9777587652206421, 0.6275945901870728, 0.13957786560058594, 0.07356548309326172, 0.2919808626174927, 0.4988694190979004, 0.5511894226074219, 0.4445497393608093, 0.5305056571960449, 0.9850046634674072, 0.6505284309387207, 0.3370053768157959, 0.9505902528762817, 0.9865317940711975, 0.9663757085800171, 0.9648101925849915, 0.283113956451416, 0.8817667961120605, 0.5345495343208313, 0.6393182277679443, 0.47200262546539307, 0.9491686224937439]\n",
            "132\n",
            "roc_auc: 0.8360807986997911\n",
            "\n",
            "\n",
            "['cont', 'exo']\n",
            "cont_train:293\n",
            "exo_train:239\n",
            "cont_val:73\n",
            "exo_val:59\n",
            "training data set_total：532\n",
            "validating data set_total：132\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.2930 Acc: 0.5000\n",
            "val Loss: 0.8511 Acc: 0.4394\n",
            "Validation loss decreased (inf --> 0.851138).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7516 Acc: 0.5451\n",
            "val Loss: 1.6489 Acc: 0.4470\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7832 Acc: 0.5658\n",
            "val Loss: 0.6814 Acc: 0.5833\n",
            "Validation loss decreased (0.851138 --> 0.681382).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.6342 Acc: 0.6729\n",
            "val Loss: 0.6465 Acc: 0.5682\n",
            "Validation loss decreased (0.681382 --> 0.646545).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.6919 Acc: 0.6297\n",
            "val Loss: 0.6329 Acc: 0.7045\n",
            "Validation loss decreased (0.646545 --> 0.632927).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.6316 Acc: 0.6711\n",
            "val Loss: 0.5539 Acc: 0.6818\n",
            "Validation loss decreased (0.632927 --> 0.553929).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.5961 Acc: 0.6748\n",
            "val Loss: 0.6885 Acc: 0.6288\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.6176 Acc: 0.6767\n",
            "val Loss: 0.6696 Acc: 0.5909\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.6105 Acc: 0.7199\n",
            "val Loss: 0.7300 Acc: 0.4773\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.6315 Acc: 0.6598\n",
            "val Loss: 0.5373 Acc: 0.7121\n",
            "Validation loss decreased (0.553929 --> 0.537313).  Saving model ...\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.6108 Acc: 0.6805\n",
            "val Loss: 0.5077 Acc: 0.7652\n",
            "Validation loss decreased (0.537313 --> 0.507718).  Saving model ...\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.6255 Acc: 0.6504\n",
            "val Loss: 0.8381 Acc: 0.7500\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.6629 Acc: 0.6823\n",
            "val Loss: 1.2103 Acc: 0.6742\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.6624 Acc: 0.6842\n",
            "val Loss: 0.6970 Acc: 0.5530\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.6295 Acc: 0.6805\n",
            "val Loss: 0.5359 Acc: 0.7576\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.5509 Acc: 0.7368\n",
            "val Loss: 0.6518 Acc: 0.7121\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.5505 Acc: 0.7124\n",
            "val Loss: 0.5986 Acc: 0.6894\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.6135 Acc: 0.6729\n",
            "val Loss: 0.7091 Acc: 0.5758\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 1.0198 Acc: 0.5658\n",
            "val Loss: 0.7454 Acc: 0.4773\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.9889 Acc: 0.5226\n",
            "val Loss: 1.5208 Acc: 0.5682\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.7339 Acc: 0.5677\n",
            "val Loss: 1.3125 Acc: 0.6136\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.7812 Acc: 0.5714\n",
            "val Loss: 0.7778 Acc: 0.6364\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.6996 Acc: 0.6184\n",
            "val Loss: 0.8769 Acc: 0.6591\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.7838 Acc: 0.5883\n",
            "val Loss: 1.5890 Acc: 0.6515\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.8028 Acc: 0.5620\n",
            "val Loss: 0.6689 Acc: 0.6515\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.7760 Acc: 0.5902\n",
            "val Loss: 0.6924 Acc: 0.6667\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 2m 22s\n",
            "Best val Acc: 0.765152\n",
            "number of images: 132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31 28 67 6\n",
            "Accuracy: 0.7424242424242424\n",
            "Precision (positive predictive value): 0.8378378378378378\n",
            "Recall (sensitivity): 0.5254237288135594\n",
            "Specificity: 0.9178082191780822\n",
            "F_value: 0.6458333333333334\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "132\n",
            "[0.3463090658187866, 0.4386674165725708, 0.3434954881668091, 0.392220675945282, 0.2899324893951416, 0.38760775327682495, 0.3896440863609314, 0.4394531846046448, 0.30506545305252075, 0.3618989586830139, 0.3233643174171448, 0.45809775590896606, 0.3365659713745117, 0.4590628743171692, 0.2931024432182312, 0.3169192671775818, 0.37272465229034424, 0.34830307960510254, 0.3411247134208679, 0.29009485244750977, 0.34271448850631714, 0.2756897807121277, 0.268251895904541, 0.6052697896957397, 0.42493146657943726, 0.2209119200706482, 0.3465768098831177, 0.36631226539611816, 0.3776127099990845, 0.387495219707489, 0.31089597940444946, 0.35988980531692505, 0.4583139419555664, 0.28870075941085815, 0.42768025398254395, 0.41141098737716675, 0.32622772455215454, 0.3176147937774658, 0.4907161593437195, 0.5076483488082886, 0.39250481128692627, 0.5758331418037415, 0.2954747676849365, 0.2692832946777344, 0.37313687801361084, 0.36841827630996704, 0.2759700417518616, 0.2406768798828125, 0.36391180753707886, 0.3644081950187683, 0.3045733571052551, 0.4092681407928467, 0.38613975048065186, 0.43970316648483276, 0.42140674591064453, 0.593254566192627, 0.34398502111434937, 0.2577196955680847, 0.37400174140930176, 0.3490830063819885, 0.30675846338272095, 0.3704531788825989, 0.36273056268692017, 0.47618794441223145, 0.3412966728210449, 0.3650591969490051, 0.3466793894767761, 0.46116191148757935, 0.3661295175552368, 0.37203818559646606, 0.3569231629371643, 0.5039839744567871, 0.7130905389785767, 0.5715664625167847, 0.7610033750534058, 0.557868242263794, 0.7718365788459778, 0.7743737697601318, 0.267112135887146, 0.3802562952041626, 0.3664237856864929, 0.4531381130218506, 0.33423060178756714, 0.4443931579589844, 0.8963403105735779, 0.24799460172653198, 0.33153802156448364, 0.5231144428253174, 0.3315170407295227, 0.4820095896720886, 0.9881558418273926, 0.964663565158844, 0.8857768774032593, 0.9999558925628662, 0.41594529151916504, 0.4286046624183655, 0.5110162496566772, 0.9099150896072388, 0.9997794032096863, 0.661156177520752, 0.9136310815811157, 0.2811405062675476, 0.44552868604660034, 0.4423137903213501, 0.30079036951065063, 0.30661243200302124, 0.49050265550613403, 0.8864164352416992, 0.5109214186668396, 0.4386669993400574, 0.9924253225326538, 0.9698330760002136, 0.9916858077049255, 0.9992504715919495, 0.25597262382507324, 0.8930773735046387, 0.9687596559524536, 0.9995321035385132, 0.41181671619415283, 0.9972748160362244, 0.2458767294883728, 0.9977914094924927, 0.26427793502807617, 0.9997441172599792, 0.43250221014022827, 0.3551141619682312, 0.7774345874786377, 0.27895891666412354, 0.3745044469833374, 0.26511162519454956, 0.8930890560150146, 0.7197152376174927]\n",
            "132\n",
            "roc_auc: 0.7385651265381936\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  0         1         2         3         4  \\\n",
            "TP                               14        32        31        35        31   \n",
            "TN                               61        52        57        62        67   \n",
            "FP                               13        21        16        11         6   \n",
            "FN                               46        28        29        24        28   \n",
            "Accuracy                   0.559701  0.631579  0.661654  0.734848  0.742424   \n",
            "Positive predictive value  0.518519  0.603774  0.659574   0.76087  0.837838   \n",
            "sensitity                  0.233333  0.533333  0.516667   0.59322  0.525424   \n",
            "specificity                0.824324  0.712329  0.780822  0.849315  0.917808   \n",
            "F-value                    0.321839  0.566372  0.579439  0.666667  0.645833   \n",
            "roc_auc                    0.439414   0.65274  0.668721  0.836081  0.738565   \n",
            "\n",
            "                                avg        std  \n",
            "TP                             28.6     7.4458  \n",
            "TN                             59.8    5.03587  \n",
            "FP                             13.4      5.004  \n",
            "FN                               31    7.69415  \n",
            "Accuracy                   0.660456  0.0759668  \n",
            "Positive predictive value   0.67705   0.126176  \n",
            "sensitity                   0.48035   0.141346  \n",
            "specificity                0.809892  0.0766895  \n",
            "F-value                    0.561986   0.137664  \n",
            "roc_auc                    0.667104   0.130928  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blaWE7qw6GjP"
      },
      "source": [
        "#**作ったフォルダの削除**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ats9BT0d6OAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132815e6-74a1-4990-ba35-e696ae88903a"
      },
      "source": [
        "dst_path = \"/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_eso\"\n",
        "directory = dst_path\n",
        "try:\n",
        "    shutil.rmtree(directory)\n",
        "except FileNotFoundError:\n",
        "    print(\"file not found\")\n",
        "    pass"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file not found\n"
          ]
        }
      ]
    }
  ]
}