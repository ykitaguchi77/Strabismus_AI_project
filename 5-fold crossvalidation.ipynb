{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled80.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPbHeHb/HZVl/2ULL0cGBSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Strabismus_AI_project/blob/main/5-fold%20crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5CVPFZ3z0ON"
      },
      "source": [
        "#**Strabismus 5-fold crossvalidation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ropSQkzzmq"
      },
      "source": [
        "\"\"\"\n",
        "CSVを作成し、ファイル名-正解-予測を記載\n",
        "\n",
        "データを5分割（division1~5)\n",
        "set1: 1がtest、2~5がtrain\n",
        "set2: 2がtest、1、3~5がtrain\n",
        "set3:\n",
        "set4: \n",
        "set5: 5がtest、1~4がtrain\n",
        "それぞれのdivisionに、exoとcontのフォルダを置いておく\n",
        "\n",
        "各セットについてトレーニング→CSVに予測の結果を記載していく\n",
        "全てのデータが揃ったら、正解-予測のデータから、正解率、感度、特異度、ROC curveの計算を行う\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyW4AJj14_og"
      },
      "source": [
        "#**Split dataset for Crossvalidation**\n",
        "trainセットを５分割、うち1つをvalセット、残りの合計をtestセットに分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "PnyR-OC25V0Q",
        "outputId": "92dce914-954c-4ea6-b3ef-6e677433aed0"
      },
      "source": [
        "import random\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "'''\n",
        "-----orig_data-----grav\n",
        "                |--cont\n",
        "↓\n",
        "↓\n",
        "\n",
        "-----dst_data[0]------dst_train[0]----grav\n",
        "  |                |               |-- cont\n",
        "  |                |--dst_val[0]------grav\n",
        "  |                                |--cont\n",
        "  |\n",
        "  |--dst_data[1]------dst_train[1]----grav\n",
        "  |                |               |-- cont\n",
        "  |                |--dst_val[1]------grav\n",
        "  |                                |--cont\n",
        "  ...\n",
        "  |--dst_data[1]------dst_train[9]----grav\n",
        "                   |               |-- cont\n",
        "                   |--dst_val[9]------grav\n",
        "                                   |--cont\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n-----orig_data-----grav\\n                |--cont\\n↓\\n↓\\n\\n-----dst_data[0]------dst_train[0]----grav\\n  |                |               |-- cont\\n  |                |--dst_val[0]------grav\\n  |                                |--cont\\n  |\\n  |--dst_data[1]------dst_train[1]----grav\\n  |                |               |-- cont\\n  |                |--dst_val[1]------grav\\n  |                                |--cont\\n  ...\\n  |--dst_data[1]------dst_train[9]----grav\\n                   |               |-- cont\\n                   |--dst_val[9]------grav\\n                                   |--cont\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-XraZlu5a5d"
      },
      "source": [
        "def get_path(orig_path, dst_path, split_num):\n",
        "    classes = os.listdir(orig_path) #クラス名を取得\n",
        "    #データの分割数を設定\n",
        "    data_list = [0]*len(classes)\n",
        "    k=0\n",
        "    for i in classes:\n",
        "        data_list[k] = glob.glob(orig_path+'/'+i+'/*')\n",
        "        k+=1\n",
        "    split_length = int(len(data_list)/split_num)\n",
        "    return data_list, classes, split_length\n",
        "\n",
        "def makefolder(orig_path, dst_path, classes):\n",
        "    #フォルダを作成\n",
        "    if os.path.exists(dst_path):\n",
        "        shutil.rmtree(dst_path)\n",
        "        print(\"Existing dataset deleted.\")\n",
        "    elif not os.path.exists(dst_path):  # ディレクトリがなかったら\n",
        "        os.mkdir(dst_path)  # 作成したいフォルダ名を作成\n",
        "        for i in range(split_num):\n",
        "            os.mkdir(dst_path+'/'+str(i))\n",
        "            os.mkdir(dst_path+'/'+str(i)+'/train')\n",
        "            os.mkdir(dst_path+'/'+str(i)+'/val')\n",
        "            for j in classes:\n",
        "                os.mkdir(dst_path+'/'+str(i)+'/train/'+j)\n",
        "                os.mkdir(dst_path+'/'+str(i)+'/val/'+j)\n",
        "\n",
        "def split_data_list(data_list, split_num):\n",
        "    split_data, dst_data, dst_train, dst_val, dst_test = [0]*split_num, [0]*split_num, [0]*split_num, [0]*split_num, [0]*split_num\n",
        "\n",
        "    #データの分割\n",
        "    split_data = list(np.array_split(data_list, split_num))\n",
        "\n",
        "    #データセット全体と分割したデータの差分を取り、dst_dataに格納\n",
        "\n",
        "    dst_data = [0] * split_num\n",
        "    for i in range(split_num):\n",
        "        dst_data[i] = [x for x in data_list if x not in split_data[i]]\n",
        "\n",
        "    #トレーニングセット、バリデーションセット、テストセットのリスト作成\n",
        "    for i in range(split_num):\n",
        "        dst_train[i] = dst_data[i]\n",
        "        dst_val[i] = split_data[i]  #テストセット\n",
        "    \n",
        "    return dst_train, dst_val\n",
        "\n",
        "def copy_to_folders(split_num, class_name, dst_train, dst_val, dst_path):\n",
        "    k=0\n",
        "    for i in range(split_num):\n",
        "        dst_path_train = dst_path +'/'+str(i)+'/train/'+class_name\n",
        "        dst_path_val = dst_path +'/'+str(i)+'/val/'+class_name\n",
        "        for p in dst_train[k]:  # 選択したファイルを目的フォルダにコピー\n",
        "            shutil.copy(p, dst_path_train)\n",
        "            #print(p)\n",
        "            print(dst_path_train)\n",
        "\n",
        "        for p in dst_val[k]:  # 選択したファイルを目的フォルダにコピー\n",
        "            shutil.copy(p, dst_path_val)\n",
        "            #print(p)    \n",
        "            print(dst_path_val)\n",
        "\n",
        "        k+=1    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmDnaPiLHxm4"
      },
      "source": [
        "#パスの設定\n",
        "orig_path = \"/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_20211111\"\n",
        "dst_path = \"/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo\"  # フォルダ名\n",
        "csv_path = dst_path + \"/img_list.csv\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIxM04cGIfQo",
        "outputId": "1580d918-dfd1-46bd-97f9-30cbadef88b0"
      },
      "source": [
        "#Dataの分割の設定\n",
        "split_num = 5  #データをいくつに分割するかを記載\n",
        "data_list, classes, split_length = get_path(orig_path, dst_path, split_num)\n",
        "print(classes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cont', 'eso', 'exo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnfDmSsjI8yX"
      },
      "source": [
        "#作成するデータのクラスを設定し直す\n",
        "#疾患群を後ろにする\n",
        "classes = ['cont', 'eso']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHp4Whs25lbn"
      },
      "source": [
        "#5-foldデータセットの作成\n",
        "#作成済みの場合には省略\n",
        "makefolder(orig_path, dst_path, classes)\n",
        "print(classes)\n",
        "k=0\n",
        "for i in range(len(classes)):\n",
        "    dst_train, dst_val = split_data_list(data_list[k], split_num)\n",
        "    print(classes[k])\n",
        "    copy_to_folders(split_num, classes[k], dst_train, dst_val, dst_path)\n",
        "    k+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SlzcdphHIA7"
      },
      "source": [
        "#**Make CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFwa0-VP-fR5"
      },
      "source": [
        "classes = os.listdir(dst_path+\"/0/val/\")\n",
        "split_num = 5\n",
        "img_num = len(glob.glob(dst_path+\"/*/val/*/*\"))\n",
        "#print(glob.glob(dst_path+\"/*/val/*/*\"))\n",
        "#print(img_num)\n",
        "\n",
        "#Make CSV file\n",
        "cols = [\"num\", \"division\", \"class\", \"prediction\"]\n",
        "df_cross = pd.DataFrame(index=list(range(img_num)), columns=cols)\n",
        "\n",
        "t=0\n",
        "for i in range(split_num):\n",
        "    for j in classes:\n",
        "        #print(str(i) + \", \"+str(j))\n",
        "        img_list = os.listdir(dst_path+\"/\"+str(i)+\"/val/\"+str(j))\n",
        "        #print(img_list)\n",
        "        for k in img_list:\n",
        "            df_cross.iloc[t,0]=str(k)\n",
        "            df_cross.iloc[t,1]=str(i)\n",
        "            df_cross.iloc[t,2]=str(j)\n",
        "            t+=1\n",
        "df_cross"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXnQMFDKL7gF"
      },
      "source": [
        "#CSV fileを保存\n",
        "csv = df_cross.to_csv(csv_path, encoding='utf_8_sig')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRZtvJGoHZIp"
      },
      "source": [
        "#**Training & evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZCjIXc4ULq1",
        "outputId": "15bf4e3d-b382-4297-ea50-6489bcc04244"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 33.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 51 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 61 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 539 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.10.0+cu111)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=61b53201556fcbd298e4e0e156f92638b0e9b15862e01909db346de0aadc0403\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiozE-KzUZPp"
      },
      "source": [
        "#Module群\n",
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == classes[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == classes[1]:\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == classes[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == classes[1]:\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n",
        "\n",
        "def makefolder(path):\n",
        "    if not os.path.exists(path):  # ディレクトリがなかったら\n",
        "        os.mkdir(path)  # 作成したいフォルダ名を作成\n",
        "\n",
        "def convnet():\n",
        "    model_ft = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ugZyB_gxU2uX",
        "outputId": "445db7c1-e025-46d4-97bd-dc781d7db013"
      },
      "source": [
        "#まとめて解析\n",
        "# 出力名を記入\n",
        "out_name = \"ResNet50_pretrained_all\"\n",
        "\n",
        "#create data_dir_list\n",
        "data_dir = dst_path\n",
        "fold = split_num\n",
        "print(str(fold)+'-fold cross validation')\n",
        "\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "#print(roc_label_list)\n",
        "\n",
        "\n",
        "\n",
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "#print(data_dir_list)\n",
        "#print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    torch.save(model_ft.state_dict(), data_dir + '/'+str(out_name)+\"_\"+str(i)+\".pth\")    #ネットワークの保存\n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, data_dir + \"/\" +str(i)+\"/val\")\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# CSVとして出力\n",
        "makefolder(data_dir + \"/crossvalidation_csv\")\n",
        "df.to_csv(data_dir + \"/crossvalidation_csv/\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "makefolder(data_dir + \"/crossvalidation_ROCfigure\")\n",
        "fig.savefig(data_dir + \"/crossvalidation_ROCfigure\" + out_name +\".png\")\n",
        "\n",
        "#Save ROC data\n",
        "makefolder(data_dir + \"/crossvalidation_ROCdata\")\n",
        "with open(data_dir + \"/crossvalidation_ROCdata\"+out_name+\".csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-fold cross validation\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/0\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/1\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/2\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/3\n",
            "/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_exo/4\n",
            "['cont', 'exo']\n",
            "cont_train:292\n",
            "exo_train:238\n",
            "cont_val:74\n",
            "exo_val:60\n",
            "training data set_total：530\n",
            "validating data set_total：134\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.8083 Acc: 0.5774\n",
            "val Loss: 1.4774 Acc: 0.4851\n",
            "Validation loss decreased (inf --> 1.477391).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.6173 Acc: 0.6472\n",
            "val Loss: 2.2700 Acc: 0.3806\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.6341 Acc: 0.6604\n",
            "val Loss: 2.2699 Acc: 0.4403\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.4798 Acc: 0.7868\n",
            "val Loss: 1.6557 Acc: 0.5672\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.3490 Acc: 0.8547\n",
            "val Loss: 5.5859 Acc: 0.4552\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.2981 Acc: 0.8830\n",
            "val Loss: 2.0811 Acc: 0.4627\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.2873 Acc: 0.8962\n",
            "val Loss: 0.9367 Acc: 0.7090\n",
            "Validation loss decreased (1.477391 --> 0.936675).  Saving model ...\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.2291 Acc: 0.9170\n",
            "val Loss: 0.3685 Acc: 0.8881\n",
            "Validation loss decreased (0.936675 --> 0.368526).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1855 Acc: 0.9340\n",
            "val Loss: 3.4356 Acc: 0.5522\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.2692 Acc: 0.9094\n",
            "val Loss: 3.3089 Acc: 0.5075\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.2933 Acc: 0.8717\n",
            "val Loss: 1.2747 Acc: 0.6493\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.2203 Acc: 0.9170\n",
            "val Loss: 0.4914 Acc: 0.7687\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.1975 Acc: 0.9245\n",
            "val Loss: 0.9826 Acc: 0.7164\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.2172 Acc: 0.9208\n",
            "val Loss: 0.3679 Acc: 0.8582\n",
            "Validation loss decreased (0.368526 --> 0.367878).  Saving model ...\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1804 Acc: 0.9321\n",
            "val Loss: 0.4521 Acc: 0.7761\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.1358 Acc: 0.9453\n",
            "val Loss: 0.3194 Acc: 0.8657\n",
            "Validation loss decreased (0.367878 --> 0.319384).  Saving model ...\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.1454 Acc: 0.9396\n",
            "val Loss: 1.3539 Acc: 0.5448\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.1498 Acc: 0.9415\n",
            "val Loss: 1.4550 Acc: 0.5299\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.1262 Acc: 0.9472\n",
            "val Loss: 0.4220 Acc: 0.8657\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.1472 Acc: 0.9528\n",
            "val Loss: 0.2722 Acc: 0.8955\n",
            "Validation loss decreased (0.319384 --> 0.272195).  Saving model ...\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.1513 Acc: 0.9340\n",
            "val Loss: 1.1458 Acc: 0.5522\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.1172 Acc: 0.9528\n",
            "val Loss: 0.3017 Acc: 0.8806\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0955 Acc: 0.9623\n",
            "val Loss: 0.2819 Acc: 0.8881\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0767 Acc: 0.9774\n",
            "val Loss: 1.1339 Acc: 0.6791\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.1594 Acc: 0.9415\n",
            "val Loss: 0.4310 Acc: 0.8582\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0834 Acc: 0.9623\n",
            "val Loss: 0.3578 Acc: 0.8806\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.1055 Acc: 0.9604\n",
            "val Loss: 3.2004 Acc: 0.5597\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.1208 Acc: 0.9566\n",
            "val Loss: 0.8364 Acc: 0.7687\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.1260 Acc: 0.9453\n",
            "val Loss: 0.3687 Acc: 0.8507\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.1171 Acc: 0.9491\n",
            "val Loss: 4.2549 Acc: 0.4552\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.0885 Acc: 0.9717\n",
            "val Loss: 0.4497 Acc: 0.8806\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0713 Acc: 0.9660\n",
            "val Loss: 4.8013 Acc: 0.4552\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0990 Acc: 0.9585\n",
            "val Loss: 2.2387 Acc: 0.5672\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0757 Acc: 0.9698\n",
            "val Loss: 0.4782 Acc: 0.8209\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0243 Acc: 0.9943\n",
            "val Loss: 0.2174 Acc: 0.9179\n",
            "Validation loss decreased (0.272195 --> 0.217388).  Saving model ...\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0466 Acc: 0.9849\n",
            "val Loss: 0.7426 Acc: 0.8358\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0974 Acc: 0.9585\n",
            "val Loss: 0.7896 Acc: 0.7612\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0947 Acc: 0.9642\n",
            "val Loss: 1.9415 Acc: 0.5448\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.0810 Acc: 0.9679\n",
            "val Loss: 3.7698 Acc: 0.4478\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.1077 Acc: 0.9585\n",
            "val Loss: 0.4852 Acc: 0.8955\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0918 Acc: 0.9717\n",
            "val Loss: 0.6556 Acc: 0.8358\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.0889 Acc: 0.9679\n",
            "val Loss: 1.4641 Acc: 0.5448\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.0948 Acc: 0.9604\n",
            "val Loss: 0.2130 Acc: 0.9104\n",
            "Validation loss decreased (0.217388 --> 0.213000).  Saving model ...\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.0547 Acc: 0.9830\n",
            "val Loss: 0.3496 Acc: 0.8582\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.0438 Acc: 0.9906\n",
            "val Loss: 0.5910 Acc: 0.7761\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.0281 Acc: 0.9906\n",
            "val Loss: 3.4128 Acc: 0.4701\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.0625 Acc: 0.9792\n",
            "val Loss: 0.3028 Acc: 0.9030\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0363 Acc: 0.9887\n",
            "val Loss: 2.1781 Acc: 0.6194\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.0369 Acc: 0.9887\n",
            "val Loss: 0.4814 Acc: 0.8433\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.0521 Acc: 0.9887\n",
            "val Loss: 0.5639 Acc: 0.8134\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Training complete in 3m 31s\n",
            "Best val Acc: 0.917910\n",
            "number of images: 134\n",
            "52 8 71 3\n",
            "Accuracy: 0.917910447761194\n",
            "Precision (positive predictive value): 0.9454545454545454\n",
            "Recall (sensitivity): 0.8666666666666667\n",
            "Specificity: 0.9594594594594594\n",
            "F_value: 0.9043478260869566\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-080b72aa3df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pth\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#ネットワークの保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecificity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_pred_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mY_TRUE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mY_SCORE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-1d699e2f9582>\u001b[0m in \u001b[0;36mcalculate_auc\u001b[0;34m(label_list, model_pred_prob)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;31m#それぞれの画像における陽性の確率についてリストを作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pred_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roc_auc: \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \"\"\"\n\u001b[1;32m    770\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 771\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [74, 134]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blaWE7qw6GjP"
      },
      "source": [
        "#**作ったフォルダの削除**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ats9BT0d6OAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132815e6-74a1-4990-ba35-e696ae88903a"
      },
      "source": [
        "dst_path = \"/content/drive/MyDrive/Deep_learning/Strabismus/Dataset_250px_crossvalidation_eso\"\n",
        "directory = dst_path\n",
        "try:\n",
        "    shutil.rmtree(directory)\n",
        "except FileNotFoundError:\n",
        "    print(\"file not found\")\n",
        "    pass"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file not found\n"
          ]
        }
      ]
    }
  ]
}